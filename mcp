# Skill: agent-configuration
Source: skills/agent-configuration/SKILL.md

---
name: agent-configuration
description: Define and manage agent files (YAML frontmatter + prompt) with models, tools, and scope priorities.
---
# Agent Configuration

## When to use
- Creating or editing agent definitions for Kode/Claude-compatible runtimes

## Workflow
1. Author agent Markdown with explicit frontmatter:
   - `agent` (identifier), `name`, `description`, `tools` (list or `*`), `model_name` (preferred over deprecated `model`).
2. Place configs by priority (low→high): built-in < .claude user < .kode user < .claude project < .kode project.
3. Hot-reload or `/agents` to apply changes; ensure fs.watch invalidation works.
4. Limit tools per agent for safety; set model_name per agent when needed.

## Anti-patterns
- Using deprecated `model` field
- Omitting tool restrictions when security matters
- Forgetting scope/priority when overlapping names

## Output
- Agent definition summary (path, tools, model_name, scope) and reload confirmation


---

# Skill: agent-orchestration
Source: skills/agent-orchestration/SKILL.md

---
name: agent-orchestration
description: Multi-agent coordination and lifecycle management based on the Kode-CLI framework.
provenance: 
  source: "partially-processed/Kode-cli/docs/agents-system.md"
  required_code: "partially-processed/agents/executor.py (Load/Execute logic)"
---

# Agent Orchestration Workflow

## 1. Agent Hierarchy
- Identify the **Primary Agent** (Task Lead).
- Assign **Sub-Agents** based on specialized domains (e.g., `git`, `tech_writer`).
- Apply the 5-tier priority system for model loading.

## 2. Dynamic Discovery
- Implement hot-reload triggers for new agent configurations.
- Map capability strings to available Python agent classes.

## 3. Communication Channel
- Maintain state shared between agents in `session_state.md`.
- Enforce strict hand-off protocols to prevent logic loops.

## Standalone Requirements
- Implementation of the `AgentExecutor` class (see `executor.py`).
- Config schema for `model_selection` and `tool_restriction`.


---

# Skill: agents-overview
Source: skills/agents-overview/SKILL.md

---
name: agents-overview
description: Describe agent manifests, orchestration patterns, and delegation conventions used by the repo.
---
# Agents Overview

## When to use
- Inspect agent manifests and decide which agent to delegate a task to
- Understand agent lifecycles, allowed tools, and model preferences

## Workflow
1. Inspect `agents/` and related manifest files for frontmatter keys like `agent`, `model_name`, and `tools`.
2. Validate that each manifest includes `entrypoint` and `timeout` semantics to avoid runaway jobs.
3. Use the cataloged agent list to route tasks and ensure ACLs for tool usage are enforced.

## Output
- A short registry of agents, their capabilities, and recommended delegation rules for orchestrators


---

# Skill: ai-asset-cataloger
Source: skills/ai-asset-cataloger/SKILL.md

---
name: ai-asset-cataloger
description: Use the system-wide AI asset inventory to locate instructions, datasets, tools, and regenerate scans when assets change.
---
# AI Asset Cataloger

## When to use
- You need to locate AI-related artifacts (instructions, skills, tools, datasets, repos) across the machine quickly
- Planning backups or audits of AI assets and want category-specific lists
- The asset catalog appears stale and must be regenerated before continuing work

## Workflow
1. Start with `ai-logic/ai_asset_summary.json` to confirm scan metadata (timestamp, counts per category, skipped prefixes). If the scan predates recent changes, schedule a rerun.
2. For targeted lookups, open the matching list in `ai-logic/ai_asset_lists/` (e.g., `instructions.tsv`, `tools.tsv`, `datasets.tsv`). Filter by `path` or `mtime` to identify assets that need review or archival.
3. When you need full fidelity, load `ai-logic/ai_asset_inventory.tsv` into a spreadsheet or pandas DataFrame and filter by `category` (comma-separated), `type`, or `note` to trace exact locations.
4. To refresh the inventory, run `python ai_asset_scan.py` from the `ai-logic` directory (sudo may be required for protected paths). The script walks `/`, skips noisy prefixes, categorizes files/dirs, and rewrites both the TSV and JSON outputs.
5. After rerunning, validate that `ai_asset_summary.json` reflects new counts and check `errors` for inaccessible paths. Update downstream documentation/backups with any new or removed assets.

## Anti-patterns
- Editing inventory TSV rows by hand instead of rerunning the scanner
- Forgetting to review the `errors` array, which can hide missing access to critical directories
- Running the scan from outside the repo root (outputs land in the wrong directory)
- Overlooking comma-separated categories when filtering, leading to missed multi-tagged assets

## Output
- Up-to-date asset inventory files (TSV + JSON) and curated per-category lists ready for archival, auditing, or downstream tooling


---

# Skill: ai-logic
Source: skills/ai-logic/SKILL.md

---
name: ai-logic
description: Patterns and helpers for implementing prompt/response pipelines and reusable LLM utilities.
---
# AI Logic

## When to use
- You need to compose multi-step prompt pipelines or reuse common LLM utilities
- Normalizing prompt templates, response parsing, or structured output extraction

## Workflow
1. Identify reusable prompt fragments and parameterize them for safety and reuse.
2. Use provided helpers to build messages, sanitize inputs, and parse structured responses (JSON, YAML).
3. Centralize retry/backoff and error-handling policies for calls to external models.

## Output
- Reusable prompt components, sanitizers, and parsers ready for agent orchestration


---

# Skill: ai-pdf-reader
Source: skills/ai-pdf-reader/SKILL.md

---
name: ai-pdf-reader
description: Convert PDFs to text/markdown and extract structured knowledge for agents and docs.
---
# AI PDF Reader

## When to use
- You need to extract text or images from PDFs for RAG ingestion
- Converting scanned PDFs with OCR to searchable markdown
- Preprocessing manuals, specs, or research PDFs before indexing

## Workflow
1. Feed `AI-pdf-reader` with a PDF path or base64 payload. It runs OCR (when needed), extracts text and images, and normalizes output to markdown snippets.
2. Cache intermediate artifacts to speed repeat runs and store mapping from pages to extracted markdown for easy citation.
3. Optionally run a summarization pass to produce short abstracts per document or per page to accelerate retrieval.

## Anti-patterns
- Sending extremely large PDFs without chunking; split into page ranges before ingestion
- Treating image-only scans as text; ensure OCR is enabled for scanned sources

## Output
- Markdown fragments, page-level metadata, base64-encoded images, and optional summaries ready for RAG or archival ingestion


---

# Skill: architect
Source: skills/generated-agents/architect/SKILL.md

name: architect
description: Architect — plans project architecture, selects templates, and records system/package dependencies.
keywords: [architect, templates, architecture]
source: partially-processed/agents/architect.py

# Architect

When to use
- When starting a new project or selecting starter templates; when defining system and package dependencies.

Workflow
1. Read current `Specification` from state.
2. Use LLM prompts (`select_templates`, `technologies`) to propose architecture and templates.
3. Validate compatibility and populate `spec.system_dependencies` and `spec.package_dependencies`.
4. Update `next_state.specification` and set `next_state.action` to the architecture step.

Dependencies
- `core.templates.registry`, `core.llm.parser`, `core.db.models.Specification`, telemetry utilities.

Output
- Updates `next_state.specification`, `next_state.action`, and telemetry tags; may create project template instances.


---

# Skill: bug-hunter
Source: skills/generated-agents/bug-hunter/SKILL.md

name: bug-hunter
description: Bug Hunter — reproduces, analyzes logs, and guides bug-hunting cycles; coordinates human testing.
keywords: [bug-hunter, debugging, logs]
source: partially-processed/agents/bug_hunter.py

# Bug Hunter

When to use
- When an iteration is in a bug-hunting state or user reports failing behavior; for log aggregation and reproduction instructions.

Workflow
1. If needed, request bug reproduction instructions via LLM (`get_bug_reproduction_instructions`).
2. Check logs using a dedicated LLM check (`check_logs`) and stream breakdowns to UI.
3. Decide whether more logs are needed or a fix is identified; update iteration status accordingly.
4. Coordinate user testing and pair programming flows when required.

Dependencies
- `core.agents.mixins.ChatWithBreakdownMixin`, `core.llm.parser.JSONParser`, telemetry and UI helpers.

Output
- Updates `next_state.current_iteration` statuses, `next_state.action`, and may append `bug_hunting_cycles` with instructions.


---

# Skill: bug-hunter-pro
Source: skills/bug-hunter-pro/SKILL.md

---
name: bug-hunter-pro
description: Drive the bug-hunting loop: reproduce, log, classify, and guide fixes with user collaboration.
---
# Bug Hunter Pro

## When to use
- Active iterations that need bug reproduction and structured debugging
- Cases where logs or user testing are required before fixing
- Sessions that might escalate to pair programming for guidance

## Workflow
1. Ensure reproduction steps exist: if missing, start a background LLM task to generate detailed test steps from current task context and user feedback, then store them.
2. Branch by iteration status:
   - Hunting for bug: build the conversation from iteration history, stream a breakdown, classify whether the problem is identified or more logs are needed, then update bug-hunting cycles and set status to either awaiting fix or awaiting logging.
   - Awaiting user test/reproduction: send test instructions (and run command if present), collect user feedback on fix status, allow pair-programming opt-in, capture additional feedback, bump attempts, and mark iteration status accordingly.
   - Start pair programming: provide an initial explanation and structured log data, then loop on user prompts (questions, tell me more, hints, done). If the user offers a solution hint, draft instructions, get approval, and queue the next fix cycle.
3. After each branch, flag iteration modifications, update project stage/status signals, and clear any pending async reproduction task once consumed.

## Anti-patterns
- Continuing without reproduction steps or ignoring the async reproduction task completion
- Failing to update bug-hunting cycles, status, or attempt counters
- Implementing human hints without explicit approval during pair programming
- Letting long conversations proceed without trimming context when needed

## Output
- Updated iteration status (awaiting fix/logging/test/pair programming), human-readable instructions or logging steps, communicated test instructions, and any user feedback captured

---

# Bug Hunter Pro Workflow

## 1. Deep Trace Collection
- Aggregate logs from across the stack (frontend, middleware, DB).
- Use regex-based filtering to isolate the specific request ID/Correlation ID.

## 2. Differential Debugging
- Compare system state between "Success Case" and "Failure Case".
- Isolate non-deterministic inputs (timing, random seeds, external API latency).

## 3. Root Cause Synthesis
- Trace logic back through the call stack until the first erroneous state mutation.
- Categorize as: Memory leak, Race condition, Logic error, or Environmental drift.

## Standalone Requirements
- Log-stream processing logic from `agents/error_handler.py`.
- Regression test suite generator from `tools/test_gen.py`.


---

# Skill: bug-report-analyzer
Source: skills/bug-report-analyzer/SKILL.md

---
name: bug-report-analyzer
description: High-fidelity review of incoming bug reports to extract reproduction steps and root causes.
keywords: [bug-report, triage, reproduction, quality-assurance, root-cause]
---

# Bug Report Analysis Workflow

## 1. Input Validation
Check for mandatory fields in the report:
- **Environment**: OS, Version, Browser, etc.
- **Expected vs. Actual**: Is the defect clearly described?
- **Reproduction**: Are there specific, repeatable steps?

## 2. Technical Evaluation
- **Severity Rating**: Does this block core functionality or is it aesthetic?
- **Root Cause Hypothesis**: Based on logs or screenshots, which module is likely failing?
- **Reproduction Proof**: Attempt to script the failure locally.

## 3. Categorization
- **Confirmed**: Bug reproduced and understood.
- **Needs Info**: Insufficient data to reproduce.
- **Duplicate**: Already tracked in another issue.

## Principle
- A bug not reproducible is a bug not yet understood. Prioritize reproduction evidence over implementation guesses.


---

# Skill: build-triage-master
Source: skills/build-triage-master/SKILL.md

---
name: build-triage-master
description: Expert triage for build failures, specifically optimized for VS Code and CI environments.
keywords: [build-failure, ci-cd, triage, compiler-error, logs]
---

# Build Triage Workflow

## 1. Evidence Collection
- **Log Isolation**: Extract the specific error block from the build output.
- **Context Search**: Identify if the failure is in source code, configuration (tsconfig/package.json), or environment.

## 2. Categorization
- **Syntax/Type Error**: Immediate fix required in source.
- **Dependency Drift**: Check `node_modules` or lockfile consistency.
- **Environmental**: Check pathing, environment variables, or toolchain versions.

## 3. Resolution Path
1. **Minimal Reproduction**: Attempt to trigger the error with a targeted command.
2. **Delta Analysis**: Review recent changes in the `diff` that preceded the failure.
3. **Cache Invalidation**: If the error logic is sound, attempt `clean` builds.

## Anti-Patterns
- Blindly changing code without locating the specific log line of failure.
- Ignoring "Warning" cascades that precede the "Error".


---

# Skill: change-notification
Source: skills/change-notification/SKILL.md

---
name: change-notification
description: Automated and manual protocols for communicating significant code/config changes to stakeholders.
keywords: [notify, slack, alerts, code-change, update]
---

# Change Notification Workflow

## 1. Significance Assessment
Determine the "Blast Radius":
- **Low**: Local refactor, no API change (Log only).
- **Medium**: UI change, dependency update (Internal notify).
- **High**: Breaking API change, DB schema migration (Critical notify).

## 2. Notification Composition
- **Summary**: Concise description of *what* changed.
- **Rationale**: The *why* behind the change.
- **Action Required**: Clear instructions for other developers (e.g., "Run `npm install`").

## 3. Propagation Channels
- Update internal `changelog` or `notices.md`.
- Trigger webhook-based channel alerts (Slack/Discord/Email).

## Anti-Patterns
- Notifying for every commit (Notification Fatigue).
- Forgetting to mention breaking changes until after deployment.


---

# Skill: chrome-screen-ai
Source: skills/chrome-screen-ai/SKILL.md

---
name: chrome-screen-ai
description: Validate and package Chrome Screen AI release assets for on-device main-content extraction and OCR features.
---
# Chrome Screen AI

## When to use
- Preparing Chrome or ChromeOS builds that must bundle on-device Screen AI capabilities
- Auditing an existing installation for the correct library/model set
- Refreshing Screen AI assets to a new manifest release (e.g., 140.14)

## Workflow
1. Review README.md to confirm the release features (main content extraction, OCR) and note the on-device privacy guarantee.
2. Confirm the release version via `manifest.json` and cross-check `_metadata/verified_contents.json`; use the treehash list to verify file integrity after copying the payload.
3. For main content extraction, consult `files_list_main_content_extraction.txt` to gather the required binaries (e.g., `libchromescreenai.so`, `screen2x_model.tflite`, `screen2x_config.pbtxt`). Stage them together so consumers can load the shared library and paired model/config.
4. For OCR, use `files_list_ocr.txt` to collect the GOCR engine assets (`gocr/`, `gocr_mobile_chrome_multiscript_2024_q4_engine.binarypb`, and supporting configs). Keep directory structure intact to satisfy runtime lookups.
5. Preserve auxiliary directories such as `aksara/` (language/layout resources) and any `.pb` label/config files referenced by the lists, then update your packaging scripts to place them alongside the shared objects with correct permissions.
6. After installation, re-run the treehash verification (step 2) to ensure nothing changed and archive the THIRD_PARTY_LICENSES bundle with your distribution notes.

## Anti-patterns
- Mixing assets from different Screen AI versions without updating `manifest.json`
- Copying only portions of the file lists (missing configs/models break runtime initialization)
- Skipping treehash validation, risking corrupted or tampered binaries

## Output
- Verified Screen AI library, models, and supporting resources staged for integration, plus licensing artifacts ready for downstream packaging


---

# Skill: claude-code
Source: skills/claude-code/SKILL.md

---
name: claude-code
description: Understand and operate Claude Code plugin and action artifacts in the repository.
---
# Claude Code

## When to use
- You need to run or adapt Claude Code plugin manifests, GitHub Actions, or local test harnesses
- Integrating Claude Code workflows into CI or local agent runs

## Workflow
1. Inspect `.claude/` manifests and action workflows for allowed tools and environment variables.
2. Validate plugin configuration with the included test harnesses before enabling in CI.
3. Use dry-run or sandbox modes when experimenting with tool ACLs or new prompts.

## Output
- Verified plugin manifests and recommended invocation steps for CI and local testing


---

# Skill: claude-code-action
Source: skills/claude-code-action/SKILL.md

---
name: claude-code-action
description: Configure and operate the Claude Code GitHub Action to answer mentions, review pull requests, and run automation prompts across repositories.
---
# Claude Code Action

## When to use
- You need Claude to react to `@claude` mentions, issue assignments, or labels inside GitHub without manual intervention
- A repository wants structured automation (reviews, doc sync, flaky-test triage) using Claude Code on self-hosted GitHub runners
- Security or compliance teams require clarity on authentication, permissions, and provider setup before enabling the action

## Workflow
1. Install Claude Code locally with `claude` and run `/install-github-app` for the quickest setup, or follow docs/setup.md to install the official GitHub App manually. For organizations that cannot install third-party apps, generate a custom app from github-app-manifest.json and capture its `APP_ID` and private key.
2. Add authentication secrets under Settings → Secrets → Actions: either `ANTHROPIC_API_KEY` for direct API usage or `CLAUDE_CODE_OAUTH_TOKEN` from `claude setup-token`. If you created a custom GitHub App, also add `APP_ID` and `APP_PRIVATE_KEY`, and plan to mint a runtime token with actions/create-github-app-token.
3. Copy examples/claude.yml (or a tailored workflow) into `.github/workflows/`. Enable the events you expect (issue comments, PR reviews, issue assignment/label) so the action can detect tag, automation, or assignment modes automatically.
4. In the workflow step, reference the action `anthropics/claude-code-action@v1` and wire secrets through inputs. Use `prompt` for automation tasks, and pass advanced switches through `claude_args` (e.g., `--model`, `--max-turns`, `--json-schema`, plugin installs). Plug in cloud flags `use_bedrock` or `use_vertex` when routing through AWS or Google, and point to `.claude/settings.json` if you need custom tool or environment policy.
5. Optional hardening: configure `additional_permissions`, commit signing (either `use_commit_signing` or `ssh_signing_key` plus bot identity), and restrict actors via `allowed_bots` or `allowed_non_write_users`. Enable plugin marketplace entries or explicit plugins (e.g., `code-review@claude-code-plugins`) when the workflow must load curated skills before running.
6. Test by commenting `@claude` on a PR or triggering the automation job. Watch the run for dependency prompts (GitHub CLI, Bun, Claude binary). Document any repository-specific prompts or claude_args so future maintainers can extend the workflow without re-discovery.

## Anti-patterns
- Committing API keys or private keys directly into the workflow instead of referencing `${{ secrets.* }}`
- Leaving deprecated inputs (`mode`, `direct_prompt`, `allowed_tools`, etc.) in migrated workflows, which causes ignored configuration or failure
- Installing plugins via the `plugins` input without granting matching permissions or ensuring GitHub CLI access for commands like `/commit-push-pr`
- Allowing non-writers or bots without reviewing the elevated risk outlined in docs/security.md

## Output
- Verified `.github/workflows/claude.yml` (or equivalent) that references anthropics/claude-code-action@v1, configured secrets, optional custom app token creation, and a run log that shows Claude reacting correctly to repository triggers


---

# Skill: claude-code-base-action
Source: skills/claude-code-base-action/SKILL.md

---
name: claude-code-base-action
description: Run Claude Code inside GitHub workflows with direct prompts, custom tool policies, and MCP settings using the base action mirror.
---
# Claude Code Base Action

## When to use
- You need fine-grained control over Claude Code executions in GitHub Actions (custom prompts, tool ACLs, MCP servers) instead of the opinionated Claude Code app
- Automation jobs should run Claude on a schedule or in bespoke pipelines where you supply the context, secrets, and follow-on steps manually
- Teams mirroring anthropics/claude-code-action/base-action and want a quick reference for required inputs and security controls

## Workflow
1. Decide how the job will be triggered (e.g., scheduled, PR opened, manual dispatch) and add a workflow step that calls `anthropics/claude-code-base-action@beta` after checking out the repo when file context is required.
2. Provide either `prompt` or `prompt_file` to seed the session. Optionally cap runtime with `max_turns`, swap models (`model`, `fallback_model`), or override prompts (`system_prompt`, `append_system_prompt`).
3. Specify tool permissions explicitly via `allowed_tools`/`disallowed_tools`. Include MCP tool names (`mcp__server__tool`) if you load MCP servers with `mcp_config`. Keep the list tight to avoid granting shell access you do not intend.
4. Inject environment and settings as needed: use `claude_env` for runtime variables (version pins, secrets, feature flags) and `settings` (path or JSON) for Claude Code permissions/hooks. Remember the action automatically enables project MCP servers; override only what you must.
5. Authenticate with either `${{ secrets.ANTHROPIC_API_KEY }}` or `${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}`. Flip `use_bedrock` or `use_vertex` when invoking providers through GitHub OIDC. Enable `use_node_cache` only if you cache Bun/Node deps deliberately.
6. Capture outputs: `execution_file` holds complete JSON conversation logs. Downstream steps (e.g., `actions/github-script`) can parse the file for summaries, comments, or structured data. Guard `show_full_output` because enabling it exposes raw content in logs.

## Anti-patterns
- Leaving `allowed_tools` empty and relying on defaults, which may block necessary commands during automation
- Combining `prompt` and `prompt_file` in the same step (the action expects only one source)
- Enabling `show_full_output` in shared repos without reviewing the security warning; secrets can leak into logs
- Forgetting to reference secrets via `${{ secrets.* }}` or storing provider credentials directly in workflow YAML

## Output
- Executed GitHub Action step that runs Claude Code with the chosen prompt, valid tool and MCP configuration, and an `execution_file` artifact ready for follow-up steps such as posting reviews or parsing structured results


---

# Skill: claude-code-plugins
Source: skills/claude-code-plugins/SKILL.md

---
name: claude-code-plugins
description: Enable and operate the official Claude Code plugin collection to extend terminal workflows with curated commands, agents, and hooks.
---
# Claude Code Plugins

## When to use
- You need Claude Code to handle git workflows, feature delivery, or PR reviews with slash commands instead of shell scripts
- A project would benefit from prebuilt hooks or agents (e.g., security reminders, frontend guidance) rather than building a plugin from scratch
- You are curating a Claude Code environment and must document which plugins are active and what operational dependencies they require

## Workflow
1. Install or update Claude Code (`curl -fsSL https://claude.ai/install.sh | bash` on macOS/Linux or the WinGet/PowerShell installer on Windows) and launch it inside the target repository with `claude`.
2. Review the bundled plugin catalog at partially-processed/claude-code/plugins/README.md to choose the module that matches the task (e.g., commit-commands for git automation, pr-review-toolkit for code review, feature-dev for guided delivery).
3. Copy the selected plugin into the project’s `.claude/plugins/` directory or point managed installations at the repo path, then confirm the plugin metadata in `<plugin>/.claude-plugin/plugin.json` lists the expected commands, agents, skills, and hooks.
4. Update `.claude/settings.json` (or managed settings) to permit the new commands/agents, add required environment variables, and declare disallowed tools if needed; for git-centric plugins ensure `gh` is installed and authenticated because `/commit-push-pr` shells out to GitHub CLI.
5. Start Claude Code and run the plugin entry command (`/commit`, `/feature-dev`, `/plugin-dev:create-plugin`, etc.), watching for dependency prompts or follow-up agents; document outcomes and any repo-specific tweaks in your operational notes so future sessions reuse the same configuration.

## Anti-patterns
- Enabling plugins without installing their external dependencies (e.g., missing GitHub CLI for commit automation or omitting project hooks referenced by `feature-dev`)
- Leaving `.claude/settings.json` unchanged so new commands remain blocked by default permissions
- Mixing experimental plugins into production profiles without isolating them in managed or per-repo settings
- Forgetting to review each plugin’s README for extra setup steps (like Agent SDK template repos for `agent-sdk-dev`)

## Output
- Activated plugin directory under `.claude/plugins/`, updated settings entries, validated slash commands or hooks, and a short run log confirming the plugin executed successfully for the target workflow


---

# Skill: code-linting
Source: skills/code-linting/SKILL.md

---
name: code-linting
description: Apply GPT Engineer's pluggable linting registry to normalize generated source files.
---
# Linting Pipeline

## When to use
- You want every generated Python asset formatted with Black before execution or diffing
- A project needs a pluggable lint registry that can grow beyond Python without rewriting call sites
- Automated runs must degrade gracefully when Black is unavailable or when code is already compliant

## Workflow
1. Instantiate `Linting()` to seed the extension map that points `.py` files at `lint_python`; additional linters can be attached to `self.linters` for other suffixes [partially-processed/core/linting.py#L33-L63](partially-processed/core/linting.py#L33-L63).
2. Call `lint_files(files_dict, config)` with a `FilesDict` bundle; the method walks each filename, normalizes the extension, and dispatches to the registered handler while preserving untouched files [partially-processed/core/linting.py#L117-L178](partially-processed/core/linting.py#L117-L178).
3. Inside the Python handler, rely on `black.format_str` to format content using any `config` overrides (like `line_length`); NothingChanged and generic exceptions fall back to the original text with informative prints [partially-processed/core/linting.py#L66-L115](partially-processed/core/linting.py#L66-L115).
4. Inspect stdout to see which files changed—`lint_files` emits per-file status so CI logs reveal formatter activity without extra hooks [partially-processed/core/linting.py#L162-L178](partially-processed/core/linting.py#L162-L178).

## Anti-patterns
- Passing `None` configs expecting defaults from Black; supply `{}` explicitly when you plan to mutate it later
- Registering async or long-running linters without adjusting the loop; `lint_files` is synchronous and will block execution
- Feeding binary or non-UTF content through Black; ensure upstream steps keep `FilesDict` text-only
- Wrapping `lint_python` in additional try/except that hides formatter failures; the handler already swallows recoverable errors

## Output
- A sanitized `FilesDict` with Python files auto-formatted, non-target files untouched, and console feedback documenting each lint decision


---

# Skill: code-monkey
Source: skills/generated-agents/code-monkey/SKILL.md

name: code-monkey
description: Code Monkey — implements code changes, processes <pythagoracode> blocks, and reviews hunks.
keywords: [code-monkey, implement, code]
source: partially-processed/agents/code_monkey.py

# Code Monkey

When to use
- When a task step requires creating or changing files (save_file steps) with auto-generated code blocks.

Workflow
1. Extract `<pythagoracode>` blocks from instructions if present.
2. Attempt Relace-based edits or use standard LLM prompts to implement changes.
3. Produce diffs/hunks and handle review cycles; apply or rework hunks as needed.

Dependencies
- `core.llm.parser.OptionalCodeBlockParser`, `core.agents.mixins.FileDiffMixin`, file/DB models.

Output
- Writes or updates files in the workspace, returns AgentResponse to indicate success or request review.


---

# Skill: code-monkey
Source: skills/code-monkey/SKILL.md

---
name: code-monkey
description: Apply task instructions to files, leverage relace when possible, and capture file metadata for downstream agents.
---
# Code Monkey

## When to use
- Developer or Troubleshooter produced implementation instructions with `<pythagoracode>` blocks
- Steps include saving a file or reworking code after review feedback
- Project files need descriptive metadata once imported or generated

## Workflow
1. For `save_file` steps, gather the target path and existing content; set UI status (creating/updating/reworking) and note review attempts.
2. Try relace first when instructions include a block for the file and an access token is available; pass the snippet through `IMPLEMENT_CHANGES_AGENT` to merge edits.
3. If relace fails/absent, call `CODE_MONKEY_AGENT` with the file content, instructions, user feedback, and optional review context to produce the new code.
4. After generation, compute diff line counts, render the diff to the UI, persist the new content, and mark the step complete. Return `INPUT_REQUIRED` when inserted code expects manual values.
5. When responding to DESCRIBE_FILES requests, fan out across un-described files using `DESCRIBE_FILES_AGENT` and store summaries/references in metadata for later discovery.

## Anti-patterns
- Skipping UI status updates, causing stale file indicators
- Falling back to relace without guarding token availability or snippet presence
- Forgetting to record review attempts or feed prior feedback back into the convo
- Leaving file metadata empty after import/creation, hindering downstream search

## Output
- Updated file contents with diffs, optional `INPUT_REQUIRED` prompts, and refreshed metadata (description, references) for described files


---

# Skill: codex-tool-catalog
Source: skills/codex-tool-catalog/SKILL.md

---
name: codex-tool-catalog
description: Maintain the Codex_Tools registry, sync new helpers, and validate that the catalog and category folders stay aligned.
---
# Codex Tool Catalog

## When to use
- A new helper script, extractor, or reference library needs to be archived under Codex_Tools
- Updating `catalog.md` summaries or verifying that every category still contains live tooling
- Running preflight checks before publishing changes to the tooling registry

## Workflow
1. Review `Codex_Tools/catalog.md` for existing categories (UnityEx build helpers, Asset extraction helpers, IL2CPP & Ghidra automation helpers, Reference libraries). Use it to decide where the new asset belongs and to confirm the description/use-case format.
2. Copy new helpers with `Codex_Tools/scripts/registry_sync.sh <category> <source> [dest-name]`. The script resolves the repo root, creates the category directory if missing, copies the asset into `Codex_Tools/Tools/<category>/`, makes it executable when applicable, and reminds you to update the catalog entry.
3. After syncing files, open `Codex_Tools/catalog.md` and add or update the bullet that describes the helper (summary + use cases). Link to any supporting notes under `Codex_Tools/notes/` (e.g., `tool_build_notes.md`, `verification-checklist.md`, `dependency-lock.md`, `versions.md`) when they affect rebuild steps or validation.
4. Run `Codex_Tools/scripts/validate_catalog.sh` to ensure required category folders exist, contain at least one helper, the References directory is present, and `catalog.md` still includes the expected section headings. Fix any errors it reports before committing.
5. When large rebuilds occur, append artifacts and attempt reports under `Codex_Tools/efforts/` so future agents can replay the workflow. Reference those snapshots inside the catalog entry to keep provenance clear.

## Anti-patterns
- Dropping files directly into `Tools/<category>` without running `registry_sync.sh` (skips executable bit and copy reminder)
- Forgetting to add the helper description to `catalog.md`, leaving future agents guessing about purpose or usage
- Skipping `validate_catalog.sh`, which allows missing directories or section headings to slip through
- Ignoring the notes/efforts directories when recording rebuild instructions or outputs

## Output
- Updated `Codex_Tools/Tools/<category>/` contents, refreshed `catalog.md` entries, passing validation script output, and optional effort records/notes for traceability


---

# Skill: coding-tools
Source: skills/coding-tools/SKILL.md

---
name: coding-tools
description: Catalog and use developer helper scripts and local CLIs found in the repo.
---
# Coding Tools

## When to use
- You want to run small utilities (formatters, generators, test runners) bundled with the repository
- A troubleshooting step needs reproducible helper commands present in `Coding_Tools` or `tools/`

## Workflow
1. Inspect `Coding_Tools` for README entries and discover available scripts.
2. Prefer invoking helpers via the provided wrapper (if present) to maintain environment consistency.
3. When adding new utilities, include short usage docs and a test harness so agents can call them safely.

## Output
- A known inventory of small, testable developer helpers with documented invocations for agent usage


---

# Skill: command-executor
Source: skills/command-executor/SKILL.md

---
name: command-executor
description: Confirm, run, and analyze shell commands with logged outputs and task step completion.
---
# Command Executor

## When to use
- A task step includes a command to run (`step["command"]`)
- Needs human confirmation or command substitution before execution
- Capturing stdout/stderr analysis for later debugging

## Workflow
1. Ensure `self.step` is set (via orchestrator) and extract command text plus optional timeout; format TL question with RUN_COMMAND strings.
2. Ask the user for confirmation; allow them to edit/replace the command inline. If declined, log skip, set EX_SKIP_COMMAND, call `complete()`, and finish.
3. When approved, capture start time, run the command through `ProcessManager.run_command` (stream output chunks to dedicated CLI source).
4. After completion, invoke the `ran_command` template, providing task steps, current task, step index, command, timeout, and captured stdout/stderr/status; parse into `CommandResult` (analysis + success flag).
5. Call `complete()` to mark the step finished in the next state, set EX_RUN_COMMAND action, and persist an `ExecLog` entry with all metadata including duration and analysis.
6. If the LLM indicates failure, bubble up via `AgentResponse.error` with full context; otherwise finish successfully.

## Anti-patterns
- Executing without user confirmation or ignoring edited command text
- Forgetting to stream output to the CLI UI source
- Skipping ExecLog persistence or next_state completion updates
- Treating unsuccessful commands as success without raising an error response

## Output
- Command execution status (EX_RUN_COMMAND or EX_SKIP_COMMAND), captured stdout/stderr, `ExecLog` entry, and analysis summarizing success or required follow-up


---

# Skill: component-architecture
Source: skills/component-architecture/SKILL.md

---
name: component-architecture
description: Design and implementation of reusable, isolated, and highly-testable system components.
keywords: [react, components, modular, interface, props]
---

# Component Architecture Workflow

## 1. Logical Scoping
- **Atomicity**: Can this be broken down further? 
- **Responsibility**: Should this component handle state, or just render it?

## 2. Interface Definition
- Set strict prop/input types (no `any`).
- Define clear events/outputs.
- Ensure "Single Source of Truth" for state management.

## 3. Isolation & Verification
- **Visual/Unit Logic**: Verify in isolation (e.g., Storybook or Component Tests).
- **Theming**: Ensure no hard-coded styles; use design tokens.

## Rules
- Components must be "pure" where possible.
- Side effects must be contained in hooks or dedicated controllers.


---

# Skill: data-engineering
Source: skills/data-engineering/SKILL.md

---
name: data-engineering
description: Architecting reliable data structures, ETL pipelines, and high-performance indexing strategies.
keywords: [database, schema, etl, partition, indexing]
---

# Data Engineering Workflow

## 1. Schema Design
- **Normalization**: Reduce redundancy while maintaining performance.
- **Indexing**: Define primary and composite keys based on access patterns.

## 2. ETL & Processing
- **Extraction**: Efficient fetching from sources.
- **Transformation**: Clean and validate data types.
- **Loading**: Atomic writes to preserve integrity.

## 3. Maintenance Protocols
- **Partioning**: Strategy for handling time-series or high-volume data.
- **Integrity Checks**: Periodic checksums and constraint verification.

## Anti-Patterns
- Storing blobs in relational tables without a storage fallback.
- Design neglect: lack of indexes on high-frequency query columns.


---

# Skill: deep-planning
Source: skills/deep-planning/SKILL.md

---
name: deep-planning
description: Multi-stage planning with mandatory stakeholder clarification for complex projects.
keywords: [ambiguous, complex-architecture, multi-step, clarify, plan]
---

# Deep Planning Workflow

## Phase 1: Clarification
Identify and resolve all logical gaps before committing to a plan.
1. **Ambiguity Scan**: List all undefined variables, missing requirements, or technical unknowns.
2. **Clarification Requests**: Phrase specific questions for the user to minimize "guesswork."
3. **Consensus Check**: Ensure user confirms the inferred constraints.

## Phase 2: Structural Planning
1. **Dependency Analysis**: Map the critical path of implementation.
2. **Failure Analysis**: Identify potential edge cases and mitigation strategies.
3. **Iterative Breakdown**: Create a task list where each item is verifiable and scoped to < 1 hour.

## Anti-Patterns
- Starting implementation with unresolved "must-fix" questions.
- Ignoring third-party library constraints until execution.


---

# Skill: developer
Source: skills/generated-agents/developer/SKILL.md

name: developer
description: Developer — breaks down tasks/iterations into executable steps and coordinates execution.
keywords: [developer, breakdown, implement]
source: partially-processed/agents/developer.py

# Developer

When to use
- When a task needs stepwise implementation, iteration breakdowns, or when running utility functions.

Workflow
1. Determine if utility function step or normal task; check for unfinished tasks/iterations.
2. Ask user to execute or run task as needed, get relevant files, and call parsing LLM prompts to produce `TaskSteps`.
3. Set next steps in state and trigger execution flows (Executor/CodeMonkey) as appropriate.

Dependencies
- `core.agents.mixins` (ChatWithBreakdownMixin, RelevantFilesMixin), LLM parsers, telemetry.

Output
- Writes `next_state.steps`, sets `next_state.action` to execution/review states, and flags modified tasks.


---

# Skill: diff-validator
Source: skills/diff-validator/SKILL.md

---
name: diff-validator
description: Validate and repair GPT Engineer diff hunks before applying them to source trees.
---
# Diff Validation Workflow

## When to use
- A downstream tool produced patch hunks that may be misaligned or contain hallucinated context
- You need to sanitize diffs before `FilesDict.apply()` or VCS patching to avoid corrupting the working tree
- An LLM returned review comments as additions and you must reclassify them without rerunning generation

## Workflow
1. Build `Hunk` instances with explicit `(label, line)` tuples and original offsets; the constructor tallies retain/add/remove counts so later length corrections stay consistent [partially-processed/core/diff.py#L44-L198](partially-processed/core/diff.py#L44-L198).
2. Call `Diff.validate_and_correct(lines_dict)` with a 1-indexed mapping of pristine lines. It walks hunks sequentially, trimming previously verified regions to keep search windows tight [partially-processed/core/diff.py#L312-L378](partially-processed/core/diff.py#L312-L378).
3. Inside each hunk, let `find_start_line` recover a trustworthy anchor if the incoming start offset is wrong; comment-only lines are downgraded to `ADD` so validation can continue [partially-processed/core/diff.py#L133-L199](partially-processed/core/diff.py#L133-L199).
4. Use `validate_lines` to auto-heal gaps: missing original lines are injected via `add_retained_line`, while spurious edits are dropped with `pop_line` based on rolling similarity ratios [partially-processed/core/diff.py#L200-L286](partially-processed/core/diff.py#L200-L286).
5. Fall back to the lightweight similarity helpers when tuning thresholds; both `is_similar` and `count_ratio` ignore whitespace and case, making them robust against formatter drift [partially-processed/core/diff.py#L381-L419](partially-processed/core/diff.py#L381-L419).

## Anti-patterns
- Feeding zero-retain hunks from new files into legacy patches; mark the whole diff as new instead of forcing validation
- Passing raw file strings without converting to ordered, 1-indexed dictionaries; validation logic assumes that structure
- Overriding `forward_block_len` to tiny sizes—similarity scoring collapses and produces false removals
- Ignoring the returned `problems` list; discarded hunks are removed from the diff silently otherwise

## Output
- Cleaned diff hunks ready for patch application, with corrected offsets, balanced retain/add/remove counts, and a diagnostics trail identifying any hunks that were unsalvageable


---

# Skill: disk-execution-env
Source: skills/disk-execution-env/SKILL.md

---
name: disk-execution-env
description: Upload generated files to a disk-backed working tree and execute commands with live stdout/stderr streaming.
---
# Disk Execution Environment

## When to use
- GPT Engineer or supporting tools must run generated entrypoints or tests inside an isolated working directory
- You want a reusable helper for pushing/pulling `FilesDict` data before executing shell commands
- Interactive sessions need live stdout/stderr streaming with timeout handling and graceful interruption

## Workflow
1. Construct `DiskExecutionEnv(path=None)`. When `path` is omitted it creates a temporary working dir via `FileStore`; providing a path reuses an existing tree.
2. Call `.upload(files_dict)` to push the current `FilesDict` snapshot into the working directory. Subsequent pulls via `.download()` return the on-disk state so you can persist command-side edits.
3. Use `.popen(command)` when you need a raw `subprocess.Popen` handle (for long-running commands you manage manually). stdout/stderr are pipe-attached for downstream reading.
4. For managed runs, use `.run(command, timeout=None)`. The helper prints `--- Start of run ---`, echoes the command, then streams stdout/stderr line-by-line until completion. Set `timeout` in seconds to auto-abort long tasks; a `TimeoutError` is raised and the process is killed.
5. If the user hits Ctrl+C, the loop catches `KeyboardInterrupt`, kills the subprocess, prints a stop message, and exits cleanly without crashing the caller.

## Anti-patterns
- Uploading large `FilesDict` blobs repeatedly instead of reusing the working tree; use `.download()` and mutate files in place when possible
- Forgetting to set `text=True` on custom `popen` usage when expecting string output (the convenience `.run` already sets it)
- Ignoring return code from `.run`; the helper returns `(stdout, stderr, returncode)` so callers should inspect `returncode` before assuming success
- Running interactive shell commands that require stdin; this environment does not forward user input beyond Ctrl+C

## Output
- A working directory populated from the latest `FilesDict`, streaming command logs to stdout, and collected `(stdout, stderr, returncode)` tuples for subsequent decision-making


---

# Skill: documentation-update
Source: skills/documentation-update/SKILL.md

---
name: documentation-update
description: Ensures technical instructions and system documentation remain in sync with evolving codebases.
keywords: [documentation, instructions, README, sync, technical-debt]
---

# Documentation Update Workflow

## 1. Trigger Identification
Documentation must be updated when:
- Internal logic changes (e.g., a new state variable is added).
- External APIs or dependencies are modified.
- User feedback indicates ambiguity in existing guides.

## 2. Sync Procedure
1. **Locate**: Identify all files referencing the changed feature (use `grep` / search).
2. **Review**: Compare current code behavior against documented behavior.
3. **Edit**: Standardize terminology and update code snippets to match the literal implementation.

## 3. Verification
- Follow the updated instructions from a "clean slate" to ensure no missed steps.
- Verify linked `README.md` or `.prompt.md` files still flow logically.

## Anti-Patterns
- Updating code logic but delaying the documentation update.
- Using vague language (e.g., "the settings file") instead of absolute paths.


---

# Skill: environment-setup
Source: skills/environment-setup/SKILL.md

---
name: environment-setup
description: Systematic initialization of project environments, dependencies, and configurations.
keywords: [init, bootstrap, environment, config, dependencies]
---

# Environment Setup Workflow

## Phase 1: Dependency Audit
- Check required runtimes (Node, Python, Go) and versions.
- Verify presence of package managers (`npm`, `poetry`, `pip`).

## Phase 2: Configuration 
- **Secret Management**: Create `.env.template` if missing; verify required keys.
- **Toolchain**: Initialize linting/testing configs if not present.

## Phase 3: Validation
1. Install base dependencies.
2. Execute a "smoke test" (e.g., `npm run build` or `python check_deps.py`).
3. Confirm working directory `cwd` matches expected project root.

## Anti-Patterns
- Installing global packages instead of project-local dev-dependencies.
- Checking `.env` files with secrets into git.


---

# Skill: error-handler
Source: skills/generated-agents/error-handler/SKILL.md

name: error-handler
description: Error Handler — handles errors returned by other agents, attempts recovery or requests exit.
keywords: [error-handler, recovery, debug]
source: partially-processed/agents/error_handler.py

# Error Handler

When to use
- When another agent returns an error response and recovery or extra diagnostics are required.

Workflow
1. Inspect the `prev_response` (error) and route handling based on agent type (e.g., Executor, SpecWriter).
2. For command errors, prompt user to allow debugging and create a new iteration with diagnostic info.
3. Return `AgentResponse.done` if recovered, or `AgentResponse.exit` to stop orchestrator.

Dependencies
- `core.agents.executor.Executor`, `core.agents.spec_writer.SpecWriter`, LLM prompts for debug analysis.

Output
- May create debugging iterations in `next_state.iterations`, set `next_state.steps`, or signal exit.


---

# Skill: error-handler
Source: skills/error-handler/SKILL.md

---
name: error-handler
description: Recover from agent failures, especially command errors, by debugging and queueing follow-up iterations.
---
# Error Handler

## When to use
- Any agent returns an error response to the orchestrator
- Command executions fail in `command-executor` and need structured debugging
- Spec creation failed and the run must exit cleanly

## Workflow
1. Inspect the previous agent response. Exit immediately when Spec Writer cannot finish project description; nothing else can proceed.
2. If the Executor failed, confirm with the user before debugging. Respect cancellations to avoid unwanted log churn.
3. When approved, stream the `debug` template with command metadata (steps, current task, step index, command, timeout, stdout, stderr, status code, analysis) to generate remediation instructions.
4. Append a fresh iteration to `next_state.iterations` using the debug output, mark it `IMPLEMENT_SOLUTION`, and prune incomplete steps so Developer can re-breakdown.
5. For unhandled agent types, log details and exit so the orchestrator can halt safely.

## Anti-patterns
- Continuing after Spec Writer failures instead of exiting
- Skipping the user confirmation before launching debug analysis
- Forgetting to move unfinished steps off the queue, leaving stale entries for downstream agents

## Output
- Either `EXIT` for irrecoverable errors or `DONE` with a new iteration containing remediation instructions and cleaned-up step lists


---

# Skill: executor
Source: skills/generated-agents/executor/SKILL.md

name: executor
description: Executor — runs shell commands, streams output to the UI, and analyzes results with the LLM.
keywords: [executor, run, cli]
source: partially-processed/agents/executor.py

# Executor

When to use
- When a task step includes a `command` to run locally as part of implementation or debugging.

Workflow
1. Confirm command with user (ask_question), optionally edit.
2. Run command via `ProcessManager.run_command`, stream stdout/stderr to UI.
3. Evaluate command output with an LLM (`ran_command` prompt) and return `CommandResult` analysis.
4. Log execution and record `ExecLog` via state manager.

Dependencies
- `core.proc.process_manager.ProcessManager`, `core.llm.parser.JSONParser`, `core.proc.exec_log.ExecLog`.

Output
- Persists `ExecLog` entries, sets `next_state.action` and may return AgentResponse.done or AgentResponse.error with details.


---

# Skill: external-docs
Source: skills/generated-agents/external-docs/SKILL.md

name: external-docs
description: External Documentation — discovers and stores external documentation snippets per task.
keywords: [external-docs, docs, rag]
source: partially-processed/agents/external_docs.py

# External Documentation

When to use
- When a task requires external documentation or when project complexity suggests fetching reference docs.

Workflow
1. Fetch available docsets from `EXTERNAL_DOCUMENTATION_API`.
2. Use LLM to select relevant docsets and craft search queries.
3. Fetch snippets via docs API and store into `next_state.docs` for the current task.

Dependencies
- `core.config.EXTERNAL_DOCUMENTATION_API`, `httpx`, telemetry, `core.llm.parser.JSONParser`.

Output
- Populates `next_state.docs` with selected doc snippets and metadata.


---

# Skill: external-docs
Source: skills/external-docs/SKILL.md

---
name: external-docs
description: Select helpful docsets and fetch snippets from the external documentation API for the current task.
---
# External Docs

## When to use
- A task could benefit from framework/library references beyond local files
- The current task has no stored docs in project state `docs`
- Example projects are not in play (they already encode documentation choices)

## Workflow
1. Skip doc collection when the spec references an example project; otherwise list docsets via `GET {EXTERNAL_DOCUMENTATION_API}/docsets` with the retried `httpx` client.
2. Ask the LLM (streaming) to run the `select_docset` template with available docsets and the current task, parsing the JSON reply with `SelectedDocsets`.
3. For each chosen docset, call the `create_docs_queries` template to gather targeted search phrases, again parsing with `DocQueries`.
4. Issue parallel `GET {EXTERNAL_DOCUMENTATION_API}/query` requests (AsyncClient) supplying docset keys and query lists; keep up to three snippets per query.
5. Store results on `next_state.docs` as dictionaries containing docset key, description, and snippet payload so downstream agents can cite them; also persist the full list of available docsets for transparency.

## Anti-patterns
- Attempting to fetch docsets when the API is unavailable without handling exceptions
- Writing documentation multiple times for the same task when `docs` already exists
- Dropping docset descriptions, leaving snippets without context for later agents

## Output
- Updated `next_state.docs` populated with docset metadata and snippets, ready for developer or writer agents to reference


---

# Skill: fast-planning
Source: skills/fast-planning/SKILL.md

---
name: fast-planning
description: Rapid sprint planning focusing on immediate deliverables and minimal viable logic.
keywords: [quick, sprint, hotfix, rapid-prototype, minimal]
---

# Fast Planning Workflow

## Philosophy
Speed is prioritized over exhaustive edge-case mapping. Focus on the "happy path" first.

## Workflow
1. **Core Objective**: Define the single most important outcome of this task.
2. **Minimal Path**: Identify the fewest possible file changes to achieve the objective.
3. **Immediate Steps**: List 3-5 concrete terminal commands or edits.
4. **Execution**: Proceed immediately to implementation.

## Critical Constraints
- Do not spend > 5 minutes on planning documentation.
- Use existing patterns exclusively; avoid introducing new abstractions.


---

# Skill: files-dict-manager
Source: skills/files-dict-manager/SKILL.md

---
name: files-dict-manager
description: Normalize GPT Engineer file bundles, enforce typing, and render chat-friendly snippets.
---
# FilesDict Handling

## When to use
- You must marshal generated files between agents and disk while guaranteeing filename/content typing
- A reviewer needs numbered source listings inside a chat transcript without running external formatters
- Logs should capture the raw body of each file in a deterministic, append-only layout

## Workflow
1. Populate `FilesDict` just like a dict, but rely on its overridden `__setitem__` to reject non-string filenames and payloads; pass `Path` objects when you want OS-safe conversions handled automatically [partially-processed/core/files_dict.py#L29-L53](partially-processed/core/files_dict.py#L29-L53).
2. Call `to_chat()` to generate a triple-backticked listing that iterates line numbers via `file_to_lines_dict`, ideal for system prompts or post-run summaries [partially-processed/core/files_dict.py#L55-L73](partially-processed/core/files_dict.py#L55-L73).
3. Use `to_log()` when you need verbatim dumps in artifacts—each file is prefixed with its name and followed by raw contents, enabling quick diffing later [partially-processed/core/files_dict.py#L74-L89](partially-processed/core/files_dict.py#L74-L89).
4. When constructing the numbered views yourself, reuse `file_to_lines_dict` to split content into an `OrderedDict`, keeping stable ordering even if filenames repeat [partially-processed/core/files_dict.py#L92-L115](partially-processed/core/files_dict.py#L92-L115).

## Anti-patterns
- Stuffing binary blobs into `FilesDict`; the type guard expects UTF-8 strings and will raise otherwise
- Feeding already formatted chat strings back into `to_chat()`—the helper double-wraps them and breaks downstream parsing
- Mutating the dict with `update()` on foreign mappings containing non-string values, which bypasses the type check
- Enumerating with `sorted()` before logging; you lose deliberate ordering that `OrderedDict` preserves for review

## Output
- Strongly typed file bundles ready for agent prompts, log uploads, or diff application, with consistent numbering and formatting helpers


---

# Skill: frontend
Source: skills/generated-agents/frontend/SKILL.md

name: frontend
description: Frontend — builds and iterates on the frontend, processes template outputs and auto-debug flows.
keywords: [frontend, build, ui]
source: partially-processed/agents/frontend.py

# Frontend

When to use
- To generate or continue building the frontend part of the project, including template application and UI iteration.

Workflow
1. Start frontend flow: clear logs, announce build, and call LLM template `build_frontend`.
2. Process returned code blocks, apply changes, optionally run auto-debug and iteration logic.
3. When finished, set frontend iteration flags and possibly move to backend steps.

Dependencies
- `core.llm.parser.DescriptiveCodeBlockParser`, `core.agents.git.GitMixin`, process manager for auto-debug.

Output
- Writes frontend files, updates `next_state.epics` messages and `fe_iteration_done` flags, and may update knowledge base.


---

# Skill: frontend-builder
Source: skills/frontend-builder/SKILL.md

---
name: frontend-builder
description: Drive template-based UI generation, change iterations, and auto-debug before handing off to backend work.
---
# Frontend Builder

## When to use
- The frontend epic needs initial scaffolding or follow-up iterations
- User feedback or auto-debugging must be folded into subsequent UI updates
- Swagger projects require replacing mocked endpoints with real API hooks

## Workflow
1. On the first pass, clear UI logs, announce frontend build start, stream the `build_frontend` template, and store the conversation history on the epic; wait for async template tasks to finish before processing code blocks.
2. For continued work, replay the stored convo, request further implementation (or `DONE`), and process each block: replace files or run npm commands (skipping missing scripts) while tracking retry counts and the `fe_iteration_done` flag.
3. When iterations begin, attempt auto-debug (kill dev servers, run `npm run start`, curl the UI, capture logs); surface failures for implementation or ask the user for change/bug descriptions. Gather relevant documentation via RAG when swagger-only and user input warrants it.
4. Apply relace-based edits first; if they fail, fall back to standard `build_frontend`. Mark conversation state, whether relace succeeded, and if manual iteration is in effect. Queue swagger mock removals when API files change and invoke removal flow before continuing.
5. On completion requests, finalize the epic, emit telemetry/UI logs, optionally run git commit, and prepare backend setup (logs, run command). If unfinished, continue iterating with updated messages and auto-debug attempt counters.

## Anti-patterns
- Ignoring unfinished async template tasks before saving files
- Leaving `file_paths_to_remove_mock` uncleared after processing swagger mocks
- Forgetting to guard npm scripts against missing entries or relace errors
- Ending an iteration without updating `fe_iteration_done`, conversation history, or run command

## Output
- Updated frontend files, executed npm commands, new run command/app link, iteration flags (`fe_iteration_done`, `use_relace`), and UI log streams marking progress or completion


---

# Skill: frontend-wizard
Source: skills/frontend-wizard/SKILL.md

---
name: frontend-wizard
description: Bootstrap the frontend/knowledge base, handle Swagger imports, and seed the first epic.
---
# Frontend Wizard

## When to use
- Starting a new project state that needs the initial frontend scaffolding
- Importing a Swagger/OpenAPI definition and configuring auth before work begins

## Workflow
1. Set the next_state action to FE_INIT and reset the in-memory template cache.
2. If the project type is `swagger`:
   - Loop until valid docs are provided; prompt the user, attempt upload via `rag/upload`, and retry on errors (refresh tokens on 403).
   - Once accepted, store returned `external_api_url` and type metadata.
   - Ask for the backend authentication method; for API key, validate availability, capture the key, and persist; for unsupported auth, collect telemetry opt-in.
3. For non-swagger projects, default auth to `login`.
4. Create a KnowledgeBase ORM instance with user options and attach it to the next state (ensure session association).
5. Seed `next_state.epics` with a single "Build frontend" epic (frontend source, open state) and clear additional structures as needed.
6. Return `AgentResponse.create_specification` if initialization succeeded; otherwise exit gracefully.

## Anti-patterns
- Proceeding without validating Swagger docs or repeating the upload on failure
- Ignoring token refresh prompts when uploads return 403
- Forgetting to persist authentication choices into knowledge base options
- Starting without creating the initial epic or knowledge base record

## Output
- Initialized knowledge base and epic list, stored Swagger metadata and auth options, and an action log indicating frontend setup


---

# Skill: git-automation
Source: skills/git-automation/SKILL.md

---
name: git-automation
description: Detect Git availability, initialize repositories, and capture commits with user-approved messages.
---
# Git Automation

## When to use
- Before orchestrator-driven runs that may need version control
- Right before task completion when changes should be committed

## Workflow
1. Check whether Git is installed via `git --version`; cache availability on the state manager.
2. Inspect the workspace for an existing `.git` folder; if missing, prompt the user to initialize.
   - On approval, run `git init`, write a default .gitignore, stage files, and create an initial commit when changes exist.
3. Before committing work, re-check for staged diffs using `git status --porcelain`. If clean, exit silently.
4. Ask the user if they want to commit; if yes, stage all changes and fetch a diff to seed a commit message via the LLM when none is provided.
5. Confirm or edit the suggested message with the user, then run `git commit -m` to persist. Update `state_manager.git_used` accordingly.

## Anti-patterns
- Running Git commands without verifying availability or initialization
- Writing .gitignore without handling filesystem errors
- Committing without user confirmation or ignoring clean working trees

## Output
- Initialized repository state, optional initial commit, and subsequent commits with user-verified messages


---

# Skill: github-mcp-server-cli
Source: skills/github-mcp-server-cli/SKILL.md

---
name: github-mcp-server-cli
description: Operate the GitHub MCP Server binary to launch stdio services, audit OAuth scopes, and regenerate documentation from inventory metadata.
---
# GitHub MCP Server CLI

## When to use
- You need to spin up the GitHub MCP server over stdio for local tooling or editor integration and must set toolset/feature flags correctly
- Security or IT teams want to audit which GitHub OAuth scopes are required for a chosen toolset before provisioning a PAT
- Documentation owners must refresh README and remote-server tables after tool or toolset changes land in the repository

## Workflow
1. Build or download the `github-mcp-server` binary and expose configuration through env vars prefixed with `GITHUB_` (for example `GITHUB_PERSONAL_ACCESS_TOKEN`, `GITHUB_TOOLSETS`, `GITHUB_FEATURES`). Launch the stdio transport with `github-mcp-server stdio` once the token and optional flags (`--toolsets`, `--tools`, `--features`, `--read-only`, `--dynamic-toolsets`, `--lockdown-mode`, etc.) are set.
2. When composing the flag list, remember Cobra + Viper normalization: hyphenated flags map to underscores for env overrides, and `--repo-access-cache-ttl` accepts Go duration strings (`5m`, `0s`). Leave `--toolsets` unset to accept the default bundle, or provide a comma separated list to constrain inventory.
3. To preview OAuth scope requirements without starting the server, run `github-mcp-server list-scopes` with the same flags you plan for production. Choose `--output=text` for human summaries, `--output=json` for programmatic pipelines, or `--output=summary` to emit unique scopes only.
4. Regenerate documentation after inventory updates with `github-mcp-server generate-docs`. The command rewrites README toolset/tool tables, docs/remote-server tables (including remote-only listings), and docs/tool-renaming alias tables by harvesting metadata from `pkg/github` inventory helpers.
5. Use `github-mcp-server --help` or `--version` when distributing the binary to verify the build metadata (ldflags set commit/date) and surface the shared persistent flag list to downstream operators.

## Anti-patterns
- Forgetting to set `GITHUB_PERSONAL_ACCESS_TOKEN`; the stdio command exits immediately with "GITHUB_PERSONAL_ACCESS_TOKEN not set"
- Supplying both `--toolsets=all` and specific `--tools` unintentionally; inventory narrowing may hide required tools for your workflow
- Running `generate-docs` in a dirty tree without committing upstream edits—tables will be rewritten and can conflict with manual changes
- Treating the list-scopes JSON output as authoritative without matching the flag configuration you intend to deploy

## Output
- Running stdio server session with the requested toolsets/features, scoped PAT, command logging, and cache TTL in effect; scope audit reports in text/JSON; regenerated documentation files updated to reflect the latest inventory metadata


---

# Skill: gpt-engineer-acp
Source: skills/gpt-engineer-acp/SKILL.md

---
name: gpt-engineer-acp
description: Run the gpt-engineer ACP server, generate MCP configs, and validate code with the ACP tooling pipeline.
---
# GPT Engineer ACP

## When to use
- Integrating gpt-engineer with ACP-compatible clients (GitHub Copilot, Zed/Toad, custom IDEs)
- Registering gpt-engineer as an MCP server for other agents
- Running the multi-stage validation pipeline before shipping generated code
- Exercising example clients or smoke-testing ACP endpoints

## Workflow
1. **Install & configure**: use Poetry within the repo (`poetry install`) and export provider keys (e.g., `OPENAI_API_KEY`). Optional env overrides include `GPT_ENGINEER_MODEL`, `GPT_ENGINEER_TEMPERATURE`, and `GPT_ENGINEER_DEBUG`.
2. **Start the ACP server**: run `gpte-acp --model <model> --project-path <dir>` (or `python -m gpt_engineer.acp.server --stdio`). The server manages sessions, JSON-RPC messaging, tool execution, and multimodal prompts.
3. **Exercise endpoints**: pipe JSON-RPC payloads (initialize, session/create, prompt/send, tools/execute) directly into `gpte-acp` or use the example scripts in `acp (2)`:
   - `basic_client.py` for a minimal text workflow
   - `multimodal_client.py` to send base64-encoded images alongside text
   - `streaming_client.py` to observe progress and file creation events
   - `test_acp.sh` for a shell-based regression script
4. **Generate MCP configuration**: import `create_mcp_config` from `gpt_engineer.acp.mcp_tools`, passing a target directory to emit `mcp-config.json` (or copy `acp/mcp-config.json`). Register the generated block under `mcpServers` so other agents can call `gpte-acp` via MCP.
5. **Integrate with Copilot**: copy `acp/copilot-config.json` into `.copilot/config.json`, then launch Copilot with `--acp --stdio` to bridge commands through the ACP server.
6. **Run validation pipeline**: invoke `gpte-validate` (or `python -m gpt_engineer.acp.validation <paths>`) to execute syntax checks, ruff linting, mypy type checks, and pytest tests. Review the summarized results before finalizing changes.
7. **Automate/testing**: incorporate the ACP tooling into pre-commit by copying the provided config (`.pre-commit-config-acp.yaml`) and running `pre-commit install`.

## Anti-patterns
- Launching the ACP server without setting `OPENAI_API_KEY`, leading to silent provider failures
- Forgetting to destroy sessions, leaving clients blocked on stale IDs
- Skipping the validation step after generation, allowing regressions to ship
- Copying configs without updating paths, causing MCP clients to point at invalid binaries

## Output
- Running ACP server ready for clients, generated MCP/copilot configs, executed example clients for smoke tests, and validation reports confirming code quality


---

# Skill: gpt-engineer-core
Source: skills/gpt-engineer-core/SKILL.md

---
name: gpt-engineer-core
description: Integrations, CLI hooks, and core helpers for the GPT Engineer subproject inside the repo.
---
# GPT Engineer Core

## When to use
- You need to interact with `gpt_engineer/` code, run its CLI, or adapt core helpers like `SimpleAgent` and `DiskExecutionEnv`

## Workflow
1. Locate the `gpt_engineer/` directory and identify CLI entrypoints and config files.
2. Use the provided helpers for generating code, running entrypoints, and persisting artifacts to disk.
3. Wire any custom AI backends by supplying compatible `AI` instances to agent constructors for local testing.

## Output
- Guidance for running GPT Engineer CLI flows and integrating core helpers into higher-level orchestration


---

# Skill: human-input
Source: skills/generated-agents/human-input/SKILL.md

name: human-input
description: Human Input — prompts the user for manual intervention or file edits when required by a step.
keywords: [human-input, user-interaction]
source: partially-processed/agents/human_input.py

# Human Input

When to use
- When a task step requires human intervention or file edits that cannot be automated.

Workflow
1. Present `human_intervention_description` to the user and wait for confirmation.
2. If files/lines are specified, open editor at locations and return after user action.
3. Mark step as complete in `next_state`.

Dependencies
- UI `open_editor` and `ask_question` helpers.

Output
- Completes human-intervention steps and returns `AgentResponse.done`.


---

# Skill: human-input
Source: skills/human-input/SKILL.md

---
name: human-input
description: Pause automation for human actions or missing data, then resume once the user confirms.
---
# Human Input

## When to use
- A task step requires manual review, approvals, or other human-only actions
- An earlier agent returned `INPUT_REQUIRED` with file positions to inspect
- The workflow needs to halt until the user signals readiness to continue

## Workflow
1. When resuming an `INPUT_REQUIRED` response, open each referenced file/line through `state_manager.file_system.get_full_path` and `ui.open_editor` so the user can inspect or edit.
2. Otherwise, show the `HUMAN_INTERVENTION_QUESTION` heading with the step-specific description and prompt the user with the `CONTINUE_WHEN_DONE` button set.
3. Await the confirmation button; once pressed, mark the `human_intervention` step complete in `next_state` so orchestrator can proceed.

## Anti-patterns
- Forgetting to surface the manual work description, leaving users unsure what to do
- Not opening requested files when handling `INPUT_REQUIRED`, preventing context inspection
- Continuing without updating step completion, which stalls downstream agents

## Output
- Updated state indicating the human-intervention step is finished (or editors opened when input was required), allowing automated agents to resume


---

# Skill: importer
Source: skills/importer/SKILL.md

---
name: importer
description: Guide project imports, gather entrypoints, and rewrite the specification for existing codebases.
---
# Importer

## When to use
- Spec Writer requests `IMPORT_PROJECT` to pull an external codebase into Pythagora
- A freshly imported workspace needs entrypoint discovery and spec generation
- Project complexity must reflect the imported size before planning begins

## Workflow
1. When triggered by Spec Writer, signal the UI to open the project root, instruct the user to copy files (warning about the 10k LOC soft limit), and wait for confirmation before calling `state_manager.import_files()` and committing.
2. Outside the initial import, stream the `get_entrypoints` template to find key files, filtering `current_state.files` to a relevant subset.
3. Feed those files plus `EXAMPLE_PROJECT_DESCRIPTION` into the `analyze_project` template to produce a narrative spec update.
4. Clone the existing specification, overwrite its description, and set `next_state.specification` alongside a completed "Import project" epic whose complexity depends on file count.
5. Emit the `existing-project` telemetry event capturing file and line counts plus the generated description.

## Anti-patterns
- Skipping the UI prompt before import, which can leave workspaces empty
- Rewriting the spec without cloning, leading to accidental shared-state edits
- Forgetting to adjust epic complexity based on imported size or to commit after copying files

## Output
- Imported files staged in the workspace, updated specification text, a completed import epic, and telemetry describing the imported project


---

# Skill: importer
Source: skills/generated-agents/importer/SKILL.md

name: importer
description: Importer — inspects and imports existing projects into the workspace and summarizes them.
keywords: [importer, project, analyze]
source: partially-processed/agents/importer.py

# Importer

When to use
- When onboarding an existing project into the MCP workspace for analysis or conversion.

Workflow
1. Ask user to copy project files to the project root and confirm.
2. Call `state_manager.import_files()` and analyze entry points via an LLM prompt.
3. Populate `Specification` and initial epics based on detected project structure.

Dependencies
- `core.db.models.Complexity`, `core.templates.example_project.EXAMPLE_PROJECT_DESCRIPTION`, telemetry.

Output
- Updates `next_state.specification`, `next_state.epics`, and may emit `AgentResponse.describe_files`.


---

# Skill: internal-overview
Source: skills/internal-overview/SKILL.md

---
name: internal-overview
description: Testing helpers, mocks, and internal-only utilities used by the repository.
---
# Internal Overview

## When to use
- You need to run or extend unit tests, mocks, or local API simulators in `internal/`
- Understanding the repo's test scaffolding and mock servers for development

## Workflow
1. Inspect `internal/` for test helpers, mocks, and local round-trippers.
2. Run unit or integration tests using the repository's test harness; inspect mock implementations for expected behavior.
3. When adding tests, follow existing patterns for deterministic mocks and small surface-area fixtures.

## Output
- Clear mapping of internal test helpers and guidance to extend or run local mocks


---

# Skill: issue-resolution
Source: skills/issue-resolution/SKILL.md

---
name: issue-resolution
description: Standardized lifecycle for resolving tracked issues with full traceability.
keywords: [fix, issue, github-issue, traceability, regression]
---

# Issue Resolution Workflow

## 1. Reproduction & Proof
- **Failing Case**: Write a unit test or script that fails due to the reported issue.
- **Isolate**: Strip dependencies until the minimal trigger is found.

## 2. Implementation & Traceability
- **The Fix**: Implement a targeted solution.
- **Commit Linking**: Use "Closes #123" or similar syntax in commit headers.

## 3. Regression Prevention
- Ensure the reproduction test now passes.
- Verify linked components aren't broken by the fix.

## Requirements
- Every fix must reference a specific Issue ID.
- No fix is complete without an accompanying test case.


---

# Skill: issue-triage
Source: skills/issue-triage/SKILL.md

---
name: issue-triage
description: Strategic management and categorization of the issue backlog to maintain project momentum.
keywords: [issue, triage, priority, labels, backlog]
---

# Issue Triage Workflow

## 1. First Response
- Acknowledge receipt of the issue/feature request.
- Assign initial labels based on domain (e.g., `bug`, `enhancement`, `security`).

## 2. Priority Alignment
Use a weighted matrix:
- **Critical (P0)**: System down, security vulnerability.
- **High (P1)**: Major feature broken, no workaround.
- **Medium (P2)**: Feature sub-optimal, workaround exists.
- **Low (P3)**: Minor polish, technical debt.

## 3. Lifecycle Routing
- **Unconfirmed**: Awaiting triage.
- **Backlog**: Approved but not scheduled.
- **Ready**: Scoped and ready for developer assignment.

## Best Practice
- Ensure every issue has a "Clear Problem Statement" before it moves to "Ready."


---

# Skill: kode-cli
Source: skills/kode-cli/SKILL.md

---
name: kode-cli
description: Use Kode CLI assets for policy-driven skill installation and agent-run orchestration.
---
# Kode CLI

## When to use
- You need to install or manage skill packs with the Kode CLI
- Automating agent workflows that rely on `Kode-cli` manifest files

## Workflow
1. Discover `Kode-cli` manifests and supported commands in the directory; inspect example invocations.
2. Run the CLI in a sandbox (or with `--dry-run`) to validate changes prior to applying them to a repo.
3. Use Kode CLI to produce reproducible environment setup scripts consumed by agents or CI.

## Output
- Standardized install scripts, manifest interpretations, and reproducible environment steps


---

# Skill: legacy-handler
Source: skills/legacy-handler/SKILL.md

---
name: legacy-handler
description: Close out legacy review steps without disturbing modern flows.
---
# Legacy Handler

## When to use
- Orchestrator encounters leftover `review_task` steps from older project states
- Need to maintain backward compatibility while migrating to newer agents

## Workflow
1. Inspect `self.data` for the legacy invocation context.
2. When `type` equals `review_task`, mark the corresponding step complete in `next_state` and return done.
3. Surface unexpected types via exceptions so new legacy cases are noticed quickly.

## Anti-patterns
- Silently ignoring unknown legacy types (hides migration issues)
- Performing additional mutations on state beyond completing the targeted step

## Output
- Updated step status for the legacy review task; no other state changes


---

# Skill: legacy-handler
Source: skills/generated-agents/legacy-handler/SKILL.md

name: legacy-handler
description: Legacy Handler — handles legacy 'review_task' calls and simple legacy flows.
keywords: [legacy-handler, review]
source: partially-processed/agents/legacy_handler.py

# Legacy Handler

When to use
- Internal compatibility: invoked for legacy 'review_task' type messages or similar legacy triggers.

Workflow
1. Inspect `self.data['type']` and route to legacy handling (e.g., complete review task).
2. Otherwise raise an error for unknown legacy reasons.

Dependencies
- `core.agents.base.BaseAgent` and AgentResponse utilities.

Output
- Typically completes a step (calls `next_state.complete_step('review_task')`) and returns `AgentResponse.done`.


---

# Skill: mcpcurl
Source: skills/mcpcurl/SKILL.md

---
name: mcpcurl
description: Invoke MCP servers over stdio by discovering tool schemas and issuing JSON-RPC calls from the mcpcurl helper CLI.
---
# mcpcurl CLI

## When to use
- You want to exercise an MCP server from the shell without wiring a full IDE integration
- QA needs to verify tool schemas or try specific tool invocations end-to-end using the same stdio transport Claude uses
- You are debugging tool arguments, enum validation, or JSON payload shapes before updating formal clients

## Workflow
1. Build or install the `mcpcurl` binary. Every command requires `--stdio-server-cmd`, a shell string that launches your MCP server in stdio mode (for example `"github-mcp-server stdio"` or `"bun run server.ts"`). Flags are parsed with Cobra so wrap multi-word commands in quotes.
2. Inspect available tools with `mcpcurl schema --stdio-server-cmd="..."`. The command sends a JSON-RPC `tools/list` request and prints the raw response so you can confirm tool names, descriptions, and parameter schemas.
3. Invoke a tool via dynamically generated subcommands under `mcpcurl tools <tool-name>`. mcpcurl binds flags based on the schema: strings, enums (validated in `PreRunE`), numeric ranges, booleans (only sent when explicitly set), and arrays (either `--flag value --flag value` or `--flag-json` for object arrays). Required properties are marked via `MarkFlagRequired`.
4. Provide arguments on the CLI or through environment variables named after the tool (`<TOOL>_<FLAG>=value`, with hyphens converted to underscores). mcpcurl merges flag and env sources before building the JSON-RPC payload.
5. Review responses using pretty printing (default `--pretty=true`). The CLI pretty prints JSON/JSONL outputs; disable with `--pretty=false` to see raw payloads. Errors from the MCP server bubble to stderr with full text content.

## Anti-patterns
- Omitting quotes around `--stdio-server-cmd` when it contains spaces, causing Cobra to interpret shell fragments as separate arguments
- Forgetting to set optional array/object flags using the `--flag-json` variant, leading to JSON unmarshal errors before the request is sent
- Assuming schema is cached permanently; mcpcurl fetches tools at startup only, so restart the process if the server’s schema changes mid-session
- Ignoring enum validation failures; mcpcurl guards client-side but the server will still reject invalid strings if bypassed

## Output
- JSON-RPC request/response cycles against the target MCP server, schema dumps for inspection, and human-friendly tool execution output suitable for reproducing bugs or validating new tools


---

# Skill: orchestrator
Source: skills/generated-agents/orchestrator/SKILL.md

name: orchestrator
description: Orchestrator — main control loop that selects, runs, and coordinates agents based on project state.
keywords: [orchestrator, control, loop]
source: partially-processed/agents/orchestrator.py

# Orchestrator

When to use
- Always runs as the top-level controller to decide which agent should run next and manage commit points.

Workflow
1. Initialize UI, dependencies, and ProcessManager/Executor.
2. In a loop: update stats, create appropriate agent(s) for current step, run them (serial or parallel), and handle responses.
3. Manage git initialization, template application, and final exit conditions.

Dependencies
- Imports many agents (`Architect`, `BugHunter`, `Developer`, etc.), `core.ui`, `ProcessManager`, and state manager.

Output
- Drives `next_state` commits, selects agents, and orchestrates task lifecycle; returns True on successful exit.


---

# Skill: orchestrator
Source: skills/orchestrator/SKILL.md

---
name: orchestrator
description: Coordinate agent pipeline, manage project initialization, and react to each agent’s responses.
---
# Orchestrator

## When to use
- Running the full Pythagora flow from specification through completion
- Responding to agent outputs, user interrupts, and project state changes

## Workflow
1. Bootstrap shared services: create an Executor (and reuse its ProcessManager), initialize UI channels, check for offline file changes, install dependencies, and apply project scaffolding tweaks (frontend script, package.json/vite updates, favicon, debugger).
2. If Git is enabled, detect installation and optionally initialize the repo before work starts.
3. Enter the main loop:
   - Handle redo flags by loading prior project states and restoring files.
   - Update telemetry stats and choose the next agent based on current state (spec writer, architect, developer, troubleshooter, problem solver, etc.).
   - Run the agent (or parallel agents when appropriate), gracefully handling user interrupts.
4. After each agent finishes:
   - On DONE, commit next_state to the database, refresh knowledge base artifacts, and emit project info if the spec writer just ran.
   - On INPUT_REQUIRED, bubble context to the UI.
   - On EXIT, break the loop and finalize.
5. Keep helper routines ready for dependency installation, script injection, debugging support, and legacy UI updates.

## Anti-patterns
- Neglecting offline change detection, which can overwrite user edits
- Forgetting to propagate ProcessManager/UI references to secondary agents
- Ignoring agent responses (e.g., not handling EXIT or INPUT_REQUIRED)
- Skipping Git initialization checks when orchestrator is instructed to use Git

## Output
- Coordinated sequence of agent invocations with committed project states, refreshed knowledge base info, and optional Git commits or scaffolding updates


---

# Skill: principled-implementation
Source: skills/principled-implementation/SKILL.md

---
name: principled-implementation
description: High-fidelity code implementation following strict architectural patterns.
keywords: [quality-code, implementation, clean-code, logic-rigor]
---

# Principled Implementation Workflow

## Rules of Execution
1. **Logic Alignment**: Verify that the code being written maps directly to an approved `plan`.
2. **Minimal Surface Area**: Modify the fewest lines necessary to achieve the requirement.
3. **Side-Effect Audit**: Check if changes impact downstream modules or exported APIs.

## Implementation Steps
- **Scaffold**: Insert comments as placeholders for complex logic.
- **Core Logic**: Implement the primary algorithm or data flow.
- **Validation**: Add error handling and edge-case checks.
- **Clean-up**: Remove debug logs and ensure naming consistency.

## Definition of Done
- No `TODO` comments left in the modified segment.
- Code matches the project's existing style and linting rules.


---

# Skill: problem-solver
Source: skills/generated-agents/problem-solver/SKILL.md

name: problem-solver
description: Problem Solver — generates and tries alternative solutions for recurring issues (loops).
keywords: [problem-solver, alternative-solutions]
source: partially-processed/agents/problem_solver.py

# Problem Solver

When to use
- When iterations are stuck in a loop and previous solutions didn't resolve the problem.

Workflow
1. Inspect current iteration and previously tried solutions.
2. Use LLM to generate alternative solutions and record them on the iteration.
3. Try preferred alternatives and update iteration status accordingly.

Dependencies
- `core.agents.troubleshooter.IterationPromptMixin`, `core.llm.parser`.

Output
- Updates `next_state.current_iteration` with `alternative_solutions` and sets iteration status to `PROBLEM_SOLVER` when trying solutions.


---

# Skill: problem-solver
Source: skills/problem-solver/SKILL.md

---
name: problem-solver
description: Regenerate and vet alternative solutions when iterations loop without progress.
---
# Problem Solver

## When to use
- An iteration is stuck in a loop and lists alternative solutions
- The user needs help deciding the next fix attempt after repeated failures

## Workflow
1. Inspect the current iteration: split alternative solutions into tried vs pending and reuse previous attempts for context.
2. If no pending solutions remain, call the `get_alternative_solutions` template to generate fresh options, append them to the iteration, and flag modifications.
3. Otherwise, present the pending solutions to the user as numbered buttons plus a "None" fallback; capture their choice.
4. If the user picks "None" (or cancels), mark all pending options as tried with feedback so they are not offered again.
5. When a solution is chosen, feed it (plus user feedback and Q&A) into `find_solution` to draft implementation instructions; record the resulting description, mark the selected solution as tried, increment attempt count, and set status to `PROBLEM_SOLVER`.

## Anti-patterns
- Continuing without user confirmation on which solution to try next
- Failing to mark solutions as tried or to capture user feedback, causing repeats
- Forgetting to flag iteration modifications after updating alternative solutions

## Output
- Updated iteration alternative_solutions list, new troubleshooting instructions, incremented attempt counter, and status transitioned to PROBLEM_SOLVER


---

# Skill: project-configurator
Source: skills/project-configurator/SKILL.md

---
name: project-configurator
description: Load, edit, and persist gpt-engineer.toml using the typed Config helpers.
---
# Project Config Management

## When to use
- A workspace needs to bootstrap or mutate `gpt-engineer.toml` without hand-editing TOML
- You want validation around optional `gptengineer-app` settings before syncing to the hosted service
- Automation must surface default build/test/lint commands while only persisting overrides

## Workflow
1. Model paths, run scripts, and optional app metadata with the provided dataclasses; `Config` wires them together with sensible defaults [partially-processed/core/project_config.py#L34-L110](partially-processed/core/project_config.py#L34-L110).
2. Use `Config.from_toml(path)` to read the existing document via `read_config`, returning a fully typed structure ready for inspection or mutation [partially-processed/core/project_config.py#L80-L110](partially-processed/core/project_config.py#L80-L110).
3. When mutating, call `to_dict()` to obtain a serialization-ready payload with `None` values stripped, keeping the TOML writer from emitting empty sections [partially-processed/core/project_config.py#L111-L121](partially-processed/core/project_config.py#L111-L121).
4. Persist updates through `to_toml(config_file, save=True)`; it merges only changed keys back into the on-disk document while preserving unrelated settings [partially-processed/core/project_config.py#L122-L151](partially-processed/core/project_config.py#L122-L151).
5. For fresh projects, seed the file with `example_config` or `default_config_filename` constants, then iterate with the same load/mutate/save cycle [partially-processed/core/project_config.py#L11-L31](partially-processed/core/project_config.py#L11-L31).

## Anti-patterns
- Calling `to_toml` on a non-existent path; always create or copy a template so `read_config` passes its existence assertion
- Bypassing `filter_none` by writing raw dicts—tomlkit will choke on `None` and you will lose sparse sections
- Omitting `project_id` when populating `gptengineer-app`; the constructor asserts its presence and will halt automation
- Treating `openapi` as a plain list of strings; it expects dictionaries that hydrate `_OpenApiConfig`

## Output
- A validated TOML document on disk plus structured config objects you can reuse across CLI invocations or CI jobs


---

# Skill: relevant-files-navigator
Source: skills/relevant-files-navigator/SKILL.md

---
name: relevant-files-navigator
description: Identify client/server files needed for the current task and keep state in sync.
---
# Relevant Files Navigator

## When to use
- Before breaking down a task or iteration where targeted file context matters
- Any time the agents need a refreshed list of files to inspect/edit

## Workflow
1. Kick off two async requests (client + server) using `filter_files` to propose relevant paths based on user feedback, solution description, and directory type.
2. Merge responses, keep only existing files, and append always-relevant entries from `ALWAYS_RELEVANT_FILES`.
3. Store the resulting list on both `current_state.relevant_files` and `next_state.relevant_files` so downstream agents reuse it.
4. Return immediately; callers decide how to continue once relevant files are in place.

## Anti-patterns
- Accepting file suggestions that do not exist in the workspace
- Forgetting to include default always-relevant files when appropriate
- Updating only current_state or next_state, leaving the other stale

## Output
- Updated relevant file list synchronized across current and next project state


---

# Skill: simple-agent
Source: skills/simple-agent/SKILL.md

---
name: simple-agent
description: Bootstrap GPT Engineer's default SimpleAgent to generate code, craft entrypoints, and run iterative improvements with disk-backed memory.
---
# Simple Agent Workflow

## When to use
- You want GPT Engineer to generate an initial codebase and entrypoint using the classic preprompt set
- An existing project needs iterative `improve` cycles without the full orchestrator stack
- You are packaging a lightweight agent that relies on disk storage for logs, prompts, and execution artifacts

## Workflow
1. Construct the agent with `SimpleAgent.with_default_config(path, ai=None, preprompts_holder=None)`. This wires `DiskMemory` at `memory_path(path)` and a `DiskExecutionEnv`, plus the standard `PrepromptsHolder(PREPROMPTS_PATH)`. Pass a custom `AI` instance if you need alternative models or telemetry.
2. Call `agent.init(prompt)` with a `Prompt` carrying `.text` and optional entrypoint prompt overrides. The agent first runs `gen_code` to produce source files, logging conversation history to `CODE_GEN_LOG_FILE`, then `gen_entrypoint` to build `ENTRYPOINT_FILE` bash script metadata and merges both into a `FilesDict`.
3. If you need execution, upload the returned `FilesDict` into the `DiskExecutionEnv` and run `execute_entrypoint`, which prompts for confirmation before invoking `bash ENTRYPOINT_FILE` in the working directory while streaming stdout/stderr.
4. For refinements, invoke `agent.improve(files_dict, prompt)` (or `improve_fn` directly) with the current files. The routine builds a chat with `setup_sys_prompt_existing_code`, parses diff blocks via `parse_diffs`, salvages valid hunks, and iterates up to `MAX_EDIT_REFINEMENT_STEPS` when validation fails. Logs land in `DEBUG_LOG_FILE`, `DIFF_LOG_FILE`, and `IMPROVE_LOG_FILE` under disk memory.
5. Persist or inspect artifacts by reading from `DiskMemory`: it keeps base64-encoded images, text sources, and timestamped log appendices. Call `archive_logs()` between sessions to rotate `logs/` into a timestamped archive.

## Anti-patterns
- Reusing the same temp directory across runs without clearing logs; stale `ENTRYPOINT_FILE` contents may leak into new sessions
- Skipping the confirmation prompt before executing entrypoints and blindly running generated scripts on CI environments
- Ignoring validation errors returned from `_improve_loop`; diffs that cannot be applied will be dropped, so retry counts and messages should be checked
- Constructing `SimpleAgent` without a proper `PrepromptsHolder`, leading to missing keys such as `generate` or `improve`

## Output
- Generated code/entrypoint files in a `FilesDict`, interactive execution capability via the disk execution environment, and disk-backed memory/logs capturing each conversation and diff iteration ready for later analysis


---

# Skill: Skill Development
Source: skills/skill-development/SKILL.md

---
name: Skill Development
description: This skill should be used when the user wants to "create a skill", "add a skill to plugin", "write a new skill", "improve skill description", "organize skill content", or needs guidance on skill structure, progressive disclosure, or skill development best practices for Claude Code plugins.
version: 0.1.0
---

# Skill Development for Claude Code Plugins

This skill provides guidance for creating effective skills for Claude Code plugins.

## About Skills

Skills are modular, self-contained packages that extend Claude's capabilities by providing
specialized knowledge, workflows, and tools. Think of them as "onboarding guides" for specific
domains or tasks—they transform Claude from a general-purpose agent into a specialized agent
equipped with procedural knowledge that no model can fully possess.

### What Skills Provide

1. Specialized workflows - Multi-step procedures for specific domains
2. Tool integrations - Instructions for working with specific file formats or APIs
3. Domain expertise - Company-specific knowledge, schemas, business logic
4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks

### Anatomy of a Skill

Every skill consists of a required SKILL.md file and optional bundled resources:

```
skill-name/
├── SKILL.md (required)
│   ├── YAML frontmatter metadata (required)
│   │   ├── name: (required)
│   │   └── description: (required)
│   └── Markdown instructions (required)
└── Bundled Resources (optional)
    ├── scripts/          - Executable code (Python/Bash/etc.)
    ├── references/       - Documentation intended to be loaded into context as needed
    └── assets/           - Files used in output (templates, icons, fonts, etc.)
```

#### SKILL.md (required)

**Metadata Quality:** The `name` and `description` in YAML frontmatter determine when Claude will use the skill. Be specific about what the skill does and when to use it. Use the third-person (e.g. "This skill should be used when..." instead of "Use this skill when...").

#### Bundled Resources (optional)

##### Scripts (`scripts/`)

Executable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.

- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed
- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks
- **Benefits**: Token efficient, deterministic, may be executed without loading into context
- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments

##### References (`references/`)

Documentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.

- **When to include**: For documentation that Claude should reference while working
- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications
- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides
- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed
- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md
- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skill—this keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.

##### Assets (`assets/`)

Files not intended to be loaded into context, but rather used within the output Claude produces.

- **When to include**: When the skill needs files that will be used in the final output
- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography
- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified
- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context

### Progressive Disclosure Design Principle

Skills use a three-level loading system to manage context efficiently:

1. **Metadata (name + description)** - Always in context (~100 words)
2. **SKILL.md body** - When skill triggers (<5k words)
3. **Bundled resources** - As needed by Claude (Unlimited*)

*Unlimited because scripts can be executed without reading into context window.

## Skill Creation Process

To create a skill, follow the "Skill Creation Process" in order, skipping steps only if there is a clear reason why they are not applicable.

### Step 1: Understanding the Skill with Concrete Examples

Skip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.

To create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.

For example, when building an image-editor skill, relevant questions include:

- "What functionality should the image-editor skill support? Editing, rotating, anything else?"
- "Can you give some examples of how this skill would be used?"
- "I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?"
- "What would a user say that should trigger this skill?"

To avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.

Conclude this step when there is a clear sense of the functionality the skill should support.

### Step 2: Planning the Reusable Skill Contents

To turn concrete examples into an effective skill, analyze each example by:

1. Considering how to execute on the example from scratch
2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly

Example: When building a `pdf-editor` skill to handle queries like "Help me rotate this PDF," the analysis shows:

1. Rotating a PDF requires re-writing the same code each time
2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill

Example: When designing a `frontend-webapp-builder` skill for queries like "Build me a todo app" or "Build me a dashboard to track my steps," the analysis shows:

1. Writing a frontend webapp requires the same boilerplate HTML/React each time
2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill

Example: When building a `big-query` skill to handle queries like "How many users have logged in today?" the analysis shows:

1. Querying BigQuery requires re-discovering the table schemas and relationships each time
2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill

**For Claude Code plugins:** When building a hooks skill, the analysis shows:
1. Developers repeatedly need to validate hooks.json and test hook scripts
2. `scripts/validate-hook-schema.sh` and `scripts/test-hook.sh` utilities would be helpful
3. `references/patterns.md` for detailed hook patterns to avoid bloating SKILL.md

To establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.

### Step 3: Create Skill Structure

For Claude Code plugins, create the skill directory structure:

```bash
mkdir -p plugin-name/skills/skill-name/{references,examples,scripts}
touch plugin-name/skills/skill-name/SKILL.md
```

**Note:** Unlike the generic skill-creator which uses `init_skill.py`, plugin skills are created directly in the plugin's `skills/` directory with a simpler manual structure.

### Step 4: Edit the Skill

When editing the (newly-created or existing) skill, remember that the skill is being created for another instance of Claude to use. Focus on including information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.

#### Start with Reusable Skill Contents

To begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.

Also, delete any example files and directories not needed for the skill. Create only the directories you actually need (references/, examples/, scripts/).

#### Update SKILL.md

**Writing Style:** Write the entire skill using **imperative/infinitive form** (verb-first instructions), not second person. Use objective, instructional language (e.g., "To accomplish X, do Y" rather than "You should do X" or "If you need to do X"). This maintains consistency and clarity for AI consumption.

**Description (Frontmatter):** Use third-person format with specific trigger phrases:

```yaml
---
name: Skill Name
description: This skill should be used when the user asks to "specific phrase 1", "specific phrase 2", "specific phrase 3". Include exact phrases users would say that should trigger this skill. Be concrete and specific.
version: 0.1.0
---
```

**Good description examples:**
```yaml
description: This skill should be used when the user asks to "create a hook", "add a PreToolUse hook", "validate tool use", "implement prompt-based hooks", or mentions hook events (PreToolUse, PostToolUse, Stop).
```

**Bad description examples:**
```yaml
description: Use this skill when working with hooks.  # Wrong person, vague
description: Load when user needs hook help.  # Not third person
description: Provides hook guidance.  # No trigger phrases
```

To complete SKILL.md body, answer the following questions:

1. What is the purpose of the skill, in a few sentences?
2. When should the skill be used? (Include this in frontmatter description with specific triggers)
3. In practice, how should Claude use the skill? All reusable skill contents developed above should be referenced so that Claude knows how to use them.

**Keep SKILL.md lean:** Target 1,500-2,000 words for the body. Move detailed content to references/:
- Detailed patterns → `references/patterns.md`
- Advanced techniques → `references/advanced.md`
- Migration guides → `references/migration.md`
- API references → `references/api-reference.md`

**Reference resources in SKILL.md:**
```markdown
## Additional Resources

### Reference Files

For detailed patterns and techniques, consult:
- **`references/patterns.md`** - Common patterns
- **`references/advanced.md`** - Advanced use cases

### Example Files

Working examples in `examples/`:
- **`example-script.sh`** - Working example
```

### Step 5: Validate and Test

**For plugin skills, validation is different from generic skills:**

1. **Check structure**: Skill directory in `plugin-name/skills/skill-name/`
2. **Validate SKILL.md**: Has frontmatter with name and description
3. **Check trigger phrases**: Description includes specific user queries
4. **Verify writing style**: Body uses imperative/infinitive form, not second person
5. **Test progressive disclosure**: SKILL.md is lean (~1,500-2,000 words), detailed content in references/
6. **Check references**: All referenced files exist
7. **Validate examples**: Examples are complete and correct
8. **Test scripts**: Scripts are executable and work correctly

**Use the skill-reviewer agent:**
```
Ask: "Review my skill and check if it follows best practices"
```

The skill-reviewer agent will check description quality, content organization, and progressive disclosure.

### Step 6: Iterate

After testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.

**Iteration workflow:**
1. Use the skill on real tasks
2. Notice struggles or inefficiencies
3. Identify how SKILL.md or bundled resources should be updated
4. Implement changes and test again

**Common improvements:**
- Strengthen trigger phrases in description
- Move long sections from SKILL.md to references/
- Add missing examples or scripts
- Clarify ambiguous instructions
- Add edge case handling

## Plugin-Specific Considerations

### Skill Location in Plugins

Plugin skills live in the plugin's `skills/` directory:

```
my-plugin/
├── .claude-plugin/
│   └── plugin.json
├── commands/
├── agents/
└── skills/
    └── my-skill/
        ├── SKILL.md
        ├── references/
        ├── examples/
        └── scripts/
```

### Auto-Discovery

Claude Code automatically discovers skills:
- Scans `skills/` directory
- Finds subdirectories containing `SKILL.md`
- Loads skill metadata (name + description) always
- Loads SKILL.md body when skill triggers
- Loads references/examples when needed

### No Packaging Needed

Plugin skills are distributed as part of the plugin, not as separate ZIP files. Users get skills when they install the plugin.

### Testing in Plugins

Test skills by installing plugin locally:

```bash
# Test with --plugin-dir
cc --plugin-dir /path/to/plugin

# Ask questions that should trigger the skill
# Verify skill loads correctly
```

## Examples from Plugin-Dev

Study the skills in this plugin as examples of best practices:

**hook-development skill:**
- Excellent trigger phrases: "create a hook", "add a PreToolUse hook", etc.
- Lean SKILL.md (1,651 words)
- 3 references/ files for detailed content
- 3 examples/ of working hooks
- 3 scripts/ utilities

**agent-development skill:**
- Strong triggers: "create an agent", "agent frontmatter", etc.
- Focused SKILL.md (1,438 words)
- References include the AI generation prompt from Claude Code
- Complete agent examples

**plugin-settings skill:**
- Specific triggers: "plugin settings", ".local.md files", "YAML frontmatter"
- References show real implementations (multi-agent-swarm, ralph-wiggum)
- Working parsing scripts

Each demonstrates progressive disclosure and strong triggering.

## Progressive Disclosure in Practice

### What Goes in SKILL.md

**Include (always loaded when skill triggers):**
- Core concepts and overview
- Essential procedures and workflows
- Quick reference tables
- Pointers to references/examples/scripts
- Most common use cases

**Keep under 3,000 words, ideally 1,500-2,000 words**

### What Goes in references/

**Move to references/ (loaded as needed):**
- Detailed patterns and advanced techniques
- Comprehensive API documentation
- Migration guides
- Edge cases and troubleshooting
- Extensive examples and walkthroughs

**Each reference file can be large (2,000-5,000+ words)**

### What Goes in examples/

**Working code examples:**
- Complete, runnable scripts
- Configuration files
- Template files
- Real-world usage examples

**Users can copy and adapt these directly**

### What Goes in scripts/

**Utility scripts:**
- Validation tools
- Testing helpers
- Parsing utilities
- Automation scripts

**Should be executable and documented**

## Writing Style Requirements

### Imperative/Infinitive Form

Write using verb-first instructions, not second person:

**Correct (imperative):**
```
To create a hook, define the event type.
Configure the MCP server with authentication.
Validate settings before use.
```

**Incorrect (second person):**
```
You should create a hook by defining the event type.
You need to configure the MCP server.
You must validate settings before use.
```

### Third-Person in Description

The frontmatter description must use third person:

**Correct:**
```yaml
description: This skill should be used when the user asks to "create X", "configure Y"...
```

**Incorrect:**
```yaml
description: Use this skill when you want to create X...
description: Load this skill when user asks...
```

### Objective, Instructional Language

Focus on what to do, not who should do it:

**Correct:**
```
Parse the frontmatter using sed.
Extract fields with grep.
Validate values before use.
```

**Incorrect:**
```
You can parse the frontmatter...
Claude should extract fields...
The user might validate values...
```

## Validation Checklist

Before finalizing a skill:

**Structure:**
- [ ] SKILL.md file exists with valid YAML frontmatter
- [ ] Frontmatter has `name` and `description` fields
- [ ] Markdown body is present and substantial
- [ ] Referenced files actually exist

**Description Quality:**
- [ ] Uses third person ("This skill should be used when...")
- [ ] Includes specific trigger phrases users would say
- [ ] Lists concrete scenarios ("create X", "configure Y")
- [ ] Not vague or generic

**Content Quality:**
- [ ] SKILL.md body uses imperative/infinitive form
- [ ] Body is focused and lean (1,500-2,000 words ideal, <5k max)
- [ ] Detailed content moved to references/
- [ ] Examples are complete and working
- [ ] Scripts are executable and documented

**Progressive Disclosure:**
- [ ] Core concepts in SKILL.md
- [ ] Detailed docs in references/
- [ ] Working code in examples/
- [ ] Utilities in scripts/
- [ ] SKILL.md references these resources

**Testing:**
- [ ] Skill triggers on expected user queries
- [ ] Content is helpful for intended tasks
- [ ] No duplicated information across files
- [ ] References load when needed

## Common Mistakes to Avoid

### Mistake 1: Weak Trigger Description

❌ **Bad:**
```yaml
description: Provides guidance for working with hooks.
```

**Why bad:** Vague, no specific trigger phrases, not third person

✅ **Good:**
```yaml
description: This skill should be used when the user asks to "create a hook", "add a PreToolUse hook", "validate tool use", or mentions hook events. Provides comprehensive hooks API guidance.
```

**Why good:** Third person, specific phrases, concrete scenarios

### Mistake 2: Too Much in SKILL.md

❌ **Bad:**
```
skill-name/
└── SKILL.md  (8,000 words - everything in one file)
```

**Why bad:** Bloats context when skill loads, detailed content always loaded

✅ **Good:**
```
skill-name/
├── SKILL.md  (1,800 words - core essentials)
└── references/
    ├── patterns.md (2,500 words)
    └── advanced.md (3,700 words)
```

**Why good:** Progressive disclosure, detailed content loaded only when needed

### Mistake 3: Second Person Writing

❌ **Bad:**
```markdown
You should start by reading the configuration file.
You need to validate the input.
You can use the grep tool to search.
```

**Why bad:** Second person, not imperative form

✅ **Good:**
```markdown
Start by reading the configuration file.
Validate the input before processing.
Use the grep tool to search for patterns.
```

**Why good:** Imperative form, direct instructions

### Mistake 4: Missing Resource References

❌ **Bad:**
```markdown
# SKILL.md

[Core content]

[No mention of references/ or examples/]
```

**Why bad:** Claude doesn't know references exist

✅ **Good:**
```markdown
# SKILL.md

[Core content]

## Additional Resources

### Reference Files
- **`references/patterns.md`** - Detailed patterns
- **`references/advanced.md`** - Advanced techniques

### Examples
- **`examples/script.sh`** - Working example
```

**Why good:** Claude knows where to find additional information

## Quick Reference

### Minimal Skill

```
skill-name/
└── SKILL.md
```

Good for: Simple knowledge, no complex resources needed

### Standard Skill (Recommended)

```
skill-name/
├── SKILL.md
├── references/
│   └── detailed-guide.md
└── examples/
    └── working-example.sh
```

Good for: Most plugin skills with detailed documentation

### Complete Skill

```
skill-name/
├── SKILL.md
├── references/
│   ├── patterns.md
│   └── advanced.md
├── examples/
│   ├── example1.sh
│   └── example2.json
└── scripts/
    └── validate.sh
```

Good for: Complex domains with validation utilities

## Best Practices Summary

✅ **DO:**
- Use third-person in description ("This skill should be used when...")
- Include specific trigger phrases ("create X", "configure Y")
- Keep SKILL.md lean (1,500-2,000 words)
- Use progressive disclosure (move details to references/)
- Write in imperative/infinitive form
- Reference supporting files clearly
- Provide working examples
- Create utility scripts for common operations
- Study plugin-dev's skills as templates

❌ **DON'T:**
- Use second person anywhere
- Have vague trigger conditions
- Put everything in SKILL.md (>3,000 words without references/)
- Write in second person ("You should...")
- Leave resources unreferenced
- Include broken or incomplete examples
- Skip validation

## Additional Resources

### Study These Skills

Plugin-dev's skills demonstrate best practices:
- `../hook-development/` - Progressive disclosure, utilities
- `../agent-development/` - AI-assisted creation, references
- `../mcp-integration/` - Comprehensive references
- `../plugin-settings/` - Real-world examples
- `../command-development/` - Clear critical concepts
- `../plugin-structure/` - Good organization

### Reference Files

For complete skill-creator methodology:
- **`references/skill-creator-original.md`** - Full original skill-creator content

## Implementation Workflow

To create a skill for your plugin:

1. **Understand use cases**: Identify concrete examples of skill usage
2. **Plan resources**: Determine what scripts/references/examples needed
3. **Create structure**: `mkdir -p skills/skill-name/{references,examples,scripts}`
4. **Write SKILL.md**:
   - Frontmatter with third-person description and trigger phrases
   - Lean body (1,500-2,000 words) in imperative form
   - Reference supporting files
5. **Add resources**: Create references/, examples/, scripts/ as needed
6. **Validate**: Check description, writing style, organization
7. **Test**: Verify skill loads on expected triggers
8. **Iterate**: Improve based on usage

Focus on strong trigger descriptions, progressive disclosure, and imperative writing style for effective skills that load when needed and provide targeted guidance.


---

# Skill: skill-creator
Source: skills/skill-creator/SKILL.md

---
name: skill-creator
description: Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Copilot's capabilities with specialized knowledge, workflows, or tool integrations.
metadata:
  short-description: Create or update a skill
---

# Skill Creator

This skill provides guidance for creating effective skills.

## About Skills

Skills are modular, self-contained packages that extend Copilot's capabilities by providing
specialized knowledge, workflows, and tools. Think of them as "onboarding guides" for specific
domains or tasks—they transform Copilot from a general-purpose agent into a specialized agent
equipped with procedural knowledge that no model can fully possess.

### What Skills Provide

1. Specialized workflows - Multi-step procedures for specific domains
2. Tool integrations - Instructions for working with specific file formats or APIs
3. Domain expertise - Company-specific knowledge, schemas, business logic
4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks

## Core Principles

### Concise is Key

The context window is a public good. Skills share the context window with everything else Copilot needs: system prompt, conversation history, other Skills' metadata, and the actual user request.

**Default assumption: Copilot is already very smart.** Only add context Copilot doesn't already have. Challenge each piece of information: "Does Copilot really need this explanation?" and "Does this paragraph justify its token cost?"

Prefer concise examples over verbose explanations.

### Set Appropriate Degrees of Freedom

Match the level of specificity to the task's fragility and variability:

**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.

**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.

**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.

Think of Copilot as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).

### Anatomy of a Skill

Every skill consists of a required SKILL.md file and optional bundled resources:

```
skill-name/
├── SKILL.md (required)
│   ├── YAML frontmatter metadata (required)
│   │   ├── name: (required)
│   │   └── description: (required)
│   └── Markdown instructions (required)
└── Bundled Resources (optional)
    ├── scripts/          - Executable code (Python/Bash/etc.)
    ├── references/       - Documentation intended to be loaded into context as needed
    └── assets/           - Files used in output (templates, icons, fonts, etc.)
```

#### SKILL.md (required)

Every SKILL.md consists of:

- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Copilot reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used.
- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).

#### Bundled Resources (optional)

##### Scripts (`scripts/`)

Executable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.

- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed
- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks
- **Benefits**: Token efficient, deterministic, may be executed without loading into context
- **Note**: Scripts may still need to be read by Copilot for patching or environment-specific adjustments

##### References (`references/`)

Documentation and reference material intended to be loaded as needed into context to inform Copilot's process and thinking.

- **When to include**: For documentation that Copilot should reference while working
- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications
- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides
- **Benefits**: Keeps SKILL.md lean, loaded only when Copilot determines it's needed
- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md
- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skill—this keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.

##### Assets (`assets/`)

Files not intended to be loaded into context, but rather used within the output Copilot produces.

- **When to include**: When the skill needs files that will be used in the final output
- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography
- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified
- **Benefits**: Separates output resources from documentation, enables Copilot to use files without loading them into context

#### What to Not Include in a Skill

A skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:

- README.md
- INSTALLATION_GUIDE.md
- QUICK_REFERENCE.md
- CHANGELOG.md
- etc.

The skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxiliary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.

### Progressive Disclosure Design Principle

Skills use a three-level loading system to manage context efficiently:

1. **Metadata (name + description)** - Always in context (~100 words)
2. **SKILL.md body** - When skill triggers (<5k words)
3. **Bundled resources** - As needed by Copilot (Unlimited because scripts can be executed without reading into context window)

#### Progressive Disclosure Patterns

Keep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.

**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.

**Pattern 1: High-level guide with references**

```markdown
# PDF Processing

## Quick start

Extract text with pdfplumber:
[code example]

## Advanced features

- **Form filling**: See [FORMS.md](FORMS.md) for complete guide
- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods
- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns
```

Copilot loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.

**Pattern 2: Domain-specific organization**

For Skills with multiple domains, organize content by domain to avoid loading irrelevant context:

```
bigquery-skill/
├── SKILL.md (overview and navigation)
└── reference/
    ├── finance.md (revenue, billing metrics)
    ├── sales.md (opportunities, pipeline)
    ├── product.md (API usage, features)
    └── marketing.md (campaigns, attribution)
```

When a user asks about sales metrics, Copilot only reads sales.md.

Similarly, for skills supporting multiple frameworks or variants, organize by variant:

```
cloud-deploy/
├── SKILL.md (workflow + provider selection)
└── references/
    ├── aws.md (AWS deployment patterns)
    ├── gcp.md (GCP deployment patterns)
    └── azure.md (Azure deployment patterns)
```

When the user chooses AWS, Copilot only reads aws.md.

**Pattern 3: Conditional details**

Show basic content, link to advanced content:

```markdown
# DOCX Processing

## Creating documents

Use docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).

## Editing documents

For simple edits, modify the XML directly.

**For tracked changes**: See [REDLINING.md](REDLINING.md)
**For OOXML details**: See [OOXML.md](OOXML.md)
```

Copilot reads REDLINING.md or OOXML.md only when the user needs those features.

**Important guidelines:**

- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.
- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Copilot can see the full scope when previewing.

## Skill Creation Process

Skill creation involves these steps:

1. Understand the skill with concrete examples
2. Plan reusable skill contents (scripts, references, assets)
3. Initialize the skill (run init_skill.py)
4. Edit the skill (implement resources and write SKILL.md)
5. Package the skill (run package_skill.py)
6. Iterate based on real usage

Follow these steps in order, skipping only if there is a clear reason why they are not applicable.

### Skill Naming

- Use lowercase letters, digits, and hyphens only; normalize user-provided titles to hyphen-case (e.g., "Plan Mode" -> `plan-mode`).
- When generating names, generate a name under 64 characters (letters, digits, hyphens).
- Prefer short, verb-led phrases that describe the action.
- Namespace by tool when it improves clarity or triggering (e.g., `gh-address-comments`, `linear-address-issue`).
- Name the skill folder exactly after the skill name.

### Step 1: Understanding the Skill with Concrete Examples

Skip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.

To create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.

For example, when building an image-editor skill, relevant questions include:

- "What functionality should the image-editor skill support? Editing, rotating, anything else?"
- "Can you give some examples of how this skill would be used?"
- "I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?"
- "What would a user say that should trigger this skill?"

To avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.

Conclude this step when there is a clear sense of the functionality the skill should support.

### Step 2: Planning the Reusable Skill Contents

To turn concrete examples into an effective skill, analyze each example by:

1. Considering how to execute on the example from scratch
2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly

Example: When building a `pdf-editor` skill to handle queries like "Help me rotate this PDF," the analysis shows:

1. Rotating a PDF requires re-writing the same code each time
2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill

Example: When designing a `frontend-webapp-builder` skill for queries like "Build me a todo app" or "Build me a dashboard to track my steps," the analysis shows:

1. Writing a frontend webapp requires the same boilerplate HTML/React each time
2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill

Example: When building a `big-query` skill to handle queries like "How many users have logged in today?" the analysis shows:

1. Querying BigQuery requires re-discovering the table schemas and relationships each time
2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill

To establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.

### Step 3: Initializing the Skill

At this point, it is time to actually create the skill.

Skip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.

When creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.

Usage:

```bash
scripts/init_skill.py <skill-name> --path <output-directory> [--resources scripts,references,assets] [--examples]
```

Examples:

```bash
scripts/init_skill.py my-skill --path skills/public
scripts/init_skill.py my-skill --path skills/public --resources scripts,references
scripts/init_skill.py my-skill --path skills/public --resources scripts --examples
```

The script:

- Creates the skill directory at the specified path
- Generates a SKILL.md template with proper frontmatter and TODO placeholders
- Optionally creates resource directories based on `--resources`
- Optionally adds example files when `--examples` is set

After initialization, customize the SKILL.md and add resources as needed. If you used `--examples`, replace or delete placeholder files.

### Step 4: Edit the Skill

When editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Copilot to use. Include information that would be beneficial and non-obvious to Copilot. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Copilot instance execute these tasks more effectively.

#### Learn Proven Design Patterns

Consult these helpful guides based on your skill's needs:

- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic
- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns

These files contain established best practices for effective skill design.

#### Start with Reusable Skill Contents

To begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.

Added scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.

If you used `--examples`, delete any placeholder files that are not needed for the skill. Only create resource directories that are actually required.

#### Update SKILL.md

**Writing Guidelines:** Always use imperative/infinitive form.

##### Frontmatter

Write the YAML frontmatter with `name` and `description`:

- `name`: The skill name
- `description`: This is the primary triggering mechanism for your skill, and helps Copilot understand when to use the skill.
  - Include both what the Skill does and specific triggers/contexts for when to use it.
  - Include all "when to use" information here - Not in the body. The body is only loaded after triggering, so "When to Use This Skill" sections in the body are not helpful to Copilot.
  - Example description for a `docx` skill: "Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Copilot needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks"

Ensure the frontmatter is valid YAML. Keep `name` and `description` as single-line scalars. If either could be interpreted as YAML syntax, wrap it in quotes.

Do not include any other fields in YAML frontmatter.

##### Body

Write instructions for using the skill and its bundled resources.

### Step 5: Packaging a Skill

Once development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:

```bash
scripts/package_skill.py <path/to/skill-folder>
```

Optional output directory specification:

```bash
scripts/package_skill.py <path/to/skill-folder> ./dist
```

The packaging script will:

1. **Validate** the skill automatically, checking:

   - YAML frontmatter format and required fields
   - Skill naming conventions and directory structure
   - Description completeness and quality
   - File organization and resource references

2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.

If validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.

### Step 6: Iterate

After testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.

**Iteration workflow:**

1. Use the skill on real tasks
2. Notice struggles or inefficiencies
3. Identify how SKILL.md or bundled resources should be updated
4. Implement changes and test again


---

# Skill: skill-installer
Source: skills/skill-installer/SKILL.md

---
name: skill-installer
description: Install skills into $CODEX_HOME/skills from the TonyRHouston/.github repository or other GitHub repo paths. Use when a user asks to list installable skills, install a curated skill, or install a skill from another repo (including private repos).
metadata:
  short-description: Install skills from TonyRHouston/.github or other repos
---

# Skill Installer

Helps install skills. By default these are from https://github.com/TonyRHouston/.github/tree/main/skills, but users can also provide other locations.

Use the helper scripts based on the task:
- List curated skills when the user asks what is available, or if the user uses this skill without specifying what to do.
- Install from the curated list when the user provides a skill name.
- Install from another repo when the user provides a GitHub repo/path (including private repos).

Install skills with the helper scripts.

## Communication

When listing curated skills, output approximately as follows, depending on the context of the user's request:
"""
Skills from {repo}:
1. skill-1
2. skill-2 (already installed)
3. ...
Which ones would you like installed?
"""

After installing a skill, tell the user: "Restart Copilot to pick up new skills."

## Scripts

All of these scripts use network, so when running in the sandbox, request escalation when running them.

- `scripts/list-curated-skills.py` (prints curated list with installed annotations)
- `scripts/list-curated-skills.py --format json`
- `scripts/install-skill-from-github.py --repo <owner>/<repo> --path <path/to/skill> [<path/to/skill> ...]`
- `scripts/install-skill-from-github.py --url https://github.com/<owner>/<repo>/tree/<ref>/<path>`

## Behavior and Options

- Defaults to direct download for public GitHub repos.
- If download fails with auth/permission errors, falls back to git sparse checkout.
- Aborts if the destination skill directory already exists.
- Installs into `$CODEX_HOME/skills/<skill-name>` (defaults to `~/.codex/skills`).
- Multiple `--path` values install multiple skills in one run, each named from the path basename unless `--name` is supplied.
- Options: `--ref <ref>` (default `main`), `--dest <path>`, `--method auto|download|git`.

## Notes

- Curated listing is fetched from `https://github.com/TonyRHouston/.github/tree/main/skills` via the GitHub API. If it is unavailable, explain the error and exit.
- Private GitHub repos can be accessed via existing git credentials or optional `GITHUB_TOKEN`/`GH_TOKEN` for download.
- Git fallback tries HTTPS first, then SSH.
- The core skills in this repository are already available, so no need to help users install those. If they ask, just explain this. If they insist, you can download and overwrite.
- Installed annotations come from `$CODEX_HOME/skills`.


---

# Skill: skill-judge
Source: skills/skill-judge/SKILL.md

---
name: skill-judge
description: Evaluate Agent Skill design quality against official specifications and best practices. Use when reviewing, auditing, or improving SKILL.md files and skill packages. Provides multi-dimensional scoring and actionable improvement suggestions.
---

# Skill Judge

Evaluate Agent Skills against official specifications and patterns derived from 17+ official examples.

---

## Core Philosophy

### What is a Skill?

A Skill is NOT a tutorial. A Skill is a **knowledge externalization mechanism**.

Traditional AI knowledge is locked in model parameters. To teach new capabilities:
```
Traditional: Collect data → GPU cluster → Train → Deploy new version
Cost: $10,000 - $1,000,000+
Timeline: Weeks to months
```

Skills change this:
```
Skill: Edit SKILL.md → Save → Takes effect on next invocation
Cost: $0
Timeline: Instant
```

This is the paradigm shift from "training AI" to "educating AI" — like a hot-swappable LoRA adapter that requires no training. You edit a Markdown file in natural language, and the model's behavior changes.

### The Core Formula

> **Good Skill = Expert-only Knowledge − What Claude Already Knows**

A Skill's value is measured by its **knowledge delta** — the gap between what it provides and what the model already knows.

- **Expert-only knowledge**: Decision trees, trade-offs, edge cases, anti-patterns, domain-specific thinking frameworks — things that take years of experience to accumulate
- **What Claude already knows**: Basic concepts, standard library usage, common programming patterns, general best practices

When a Skill explains "what is PDF" or "how to write a for-loop", it's compressing knowledge Claude already has. This is **token waste** — context window is a public resource shared with system prompts, conversation history, other Skills, and user requests.

### Tool vs Skill

| Concept | Essence | Function | Example |
|---------|---------|----------|---------|
| **Tool** | What model CAN do | Execute actions | bash, read_file, write_file, WebSearch |
| **Skill** | What model KNOWS how to do | Guide decisions | PDF processing, MCP building, frontend design |

Tools define capability boundaries — without bash tool, model can't execute commands.
Skills inject knowledge — without frontend-design Skill, model produces generic UI.

**The equation**:
```
General Agent + Excellent Skill = Domain Expert Agent
```

Same Claude model, different Skills loaded, becomes different experts.

### Three Types of Knowledge in Skills

When evaluating, categorize each section:

| Type | Definition | Treatment |
|------|------------|-----------|
| **Expert** | Claude genuinely doesn't know this | Must keep — this is the Skill's value |
| **Activation** | Claude knows but may not think of | Keep if brief — serves as reminder |
| **Redundant** | Claude definitely knows this | Should delete — wastes tokens |

The art of Skill design is maximizing Expert content, using Activation sparingly, and eliminating Redundant ruthlessly.

---

## Evaluation Dimensions (120 points total)

### D1: Knowledge Delta (20 points) — THE CORE DIMENSION

The most important dimension. Does the Skill add genuine expert knowledge?

| Score | Criteria |
|-------|----------|
| 0-5 | Explains basics Claude knows (what is X, how to write code, standard library tutorials) |
| 6-10 | Mixed: some expert knowledge diluted by obvious content |
| 11-15 | Mostly expert knowledge with minimal redundancy |
| 16-20 | Pure knowledge delta — every paragraph earns its tokens |

**Red flags** (instant score ≤5):
- "What is [basic concept]" sections
- Step-by-step tutorials for standard operations
- Explaining how to use common libraries
- Generic best practices ("write clean code", "handle errors")
- Definitions of industry-standard terms

**Green flags** (indicators of high knowledge delta):
- Decision trees for non-obvious choices ("when X fails, try Y because Z")
- Trade-offs only an expert would know ("A is faster but B handles edge case C")
- Edge cases from real-world experience
- "NEVER do X because [non-obvious reason]"
- Domain-specific thinking frameworks

**Evaluation questions**:
1. For each section, ask: "Does Claude already know this?"
2. If explaining something, ask: "Is this explaining TO Claude or FOR Claude?"
3. Count paragraphs that are Expert vs Activation vs Redundant

---

### D2: Mindset + Appropriate Procedures (15 points)

Does the Skill transfer expert **thinking patterns** along with **necessary domain-specific procedures**?

The difference between experts and novices isn't "knowing how to operate" — it's "how to think about the problem." But thinking patterns alone aren't enough when Claude lacks domain-specific procedural knowledge.

**Key distinction**:
| Type | Example | Value |
|------|---------|-------|
| **Thinking patterns** | "Before designing, ask: What makes this memorable?" | High — shapes decision-making |
| **Domain-specific procedures** | "OOXML workflow: unpack → edit XML → validate → pack" | High — Claude may not know this |
| **Generic procedures** | "Step 1: Open file, Step 2: Edit, Step 3: Save" | Low — Claude already knows |

| Score | Criteria |
|-------|----------|
| 0-3 | Only generic procedures Claude already knows |
| 4-7 | Has domain procedures but lacks thinking frameworks |
| 8-11 | Good balance: thinking patterns + domain-specific workflows |
| 12-15 | Expert-level: shapes thinking AND provides procedures Claude wouldn't know |

**What counts as valuable procedures**:
- Workflows Claude hasn't been trained on (new tools, proprietary systems)
- Correct ordering that's non-obvious (e.g., "validate BEFORE packing, not after")
- Critical steps that are easy to miss (e.g., "MUST recalculate formulas after editing")
- Domain-specific sequences (e.g., MCP server's 4-phase development process)

**What counts as redundant procedures**:
- Generic file operations (open, read, write, save)
- Standard programming patterns (loops, conditionals, error handling)
- Common library usage that's well-documented

**Expert thinking patterns look like**:
```markdown
Before [action], ask yourself:
- **Purpose**: What problem does this solve? Who uses it?
- **Constraints**: What are the hidden requirements?
- **Differentiation**: What makes this solution memorable?
```

**Valuable domain procedures look like**:
```markdown
### Redlining Workflow (Claude wouldn't know this sequence)
1. Convert to markdown: `pandoc --track-changes=all`
2. Map text to XML: grep for text in document.xml
3. Implement changes in batches of 3-10
4. Pack and verify: check ALL changes were applied
```

**Redundant generic procedures look like**:
```markdown
Step 1: Open the file
Step 2: Find the section
Step 3: Make the change
Step 4: Save and test
```

**The test**:
1. Does it tell Claude WHAT to think about? (thinking patterns)
2. Does it tell Claude HOW to do things it wouldn't know? (domain procedures)

A good Skill provides both when needed.

---

### D3: Anti-Pattern Quality (15 points)

Does the Skill have effective NEVER lists?

**Why this matters**: Half of expert knowledge is knowing what NOT to do. A senior designer sees purple gradient on white background and instinctively cringes — "too AI-generated." This intuition for "what absolutely not to do" comes from stepping on countless landmines.

Claude hasn't stepped on these landmines. It doesn't know Inter font is overused, doesn't know purple gradients are the signature of AI-generated content. Good Skills must explicitly state these "absolute don'ts."

| Score | Criteria |
|-------|----------|
| 0-3 | No anti-patterns mentioned |
| 4-7 | Generic warnings ("avoid errors", "be careful", "consider edge cases") |
| 8-11 | Specific NEVER list with some reasoning |
| 12-15 | Expert-grade anti-patterns with WHY — things only experience teaches |

**Expert anti-patterns** (specific + reason):
```markdown
NEVER use generic AI-generated aesthetics like:
- Overused font families (Inter, Roboto, Arial)
- Cliched color schemes (particularly purple gradients on white backgrounds)
- Predictable layouts and component patterns
- Default border-radius on everything
```

**Weak anti-patterns** (vague, no reasoning):
```markdown
Avoid making mistakes.
Be careful with edge cases.
Don't write bad code.
```

**The test**: Would an expert read the anti-pattern list and say "yes, I learned this the hard way"? Or would they say "this is obvious to everyone"?

---

### D4: Specification Compliance — Especially Description (15 points)

Does the Skill follow official format requirements? **Special focus on description quality.**

| Score | Criteria |
|-------|----------|
| 0-5 | Missing frontmatter or invalid format |
| 6-10 | Has frontmatter but description is vague or incomplete |
| 11-13 | Valid frontmatter, description has WHAT but weak on WHEN |
| 14-15 | Perfect: comprehensive description with WHAT, WHEN, and trigger keywords |

**Frontmatter requirements**:
- `name`: lowercase, alphanumeric + hyphens only, ≤64 characters
- `description`: **THE MOST CRITICAL FIELD** — determines if skill gets used at all

---

**Why description is THE MOST IMPORTANT field**:

```
┌─────────────────────────────────────────────────────────────────────┐
│  SKILL ACTIVATION FLOW                                              │
│                                                                     │
│  User Request → Agent sees ALL skill descriptions → Decides which  │
│                 (only descriptions, not bodies!)     to activate    │
│                                                                     │
│  If description doesn't match → Skill NEVER gets loaded            │
│  If description is vague → Skill might not trigger when it should  │
│  If description lacks keywords → Skill is invisible to the Agent   │
└─────────────────────────────────────────────────────────────────────┘
```

**The brutal truth**: A Skill with perfect content but poor description is **useless** — it will never be activated. The description is the **only chance** to tell the Agent "use me in these situations."

---

**Description must answer THREE questions**:

1. **WHAT**: What does this Skill do? (functionality)
2. **WHEN**: In what situations should it be used? (trigger scenarios)
3. **KEYWORDS**: What terms should trigger this Skill? (searchable terms)

**Excellent description** (all three elements):
```yaml
description: "Comprehensive document creation, editing, and analysis with support
for tracked changes, comments, formatting preservation, and text extraction.
When Claude needs to work with professional documents (.docx files) for:
(1) Creating new documents, (2) Modifying or editing content,
(3) Working with tracked changes, (4) Adding comments, or any other document tasks"
```

Analysis:
- WHAT: creation, editing, analysis, tracked changes, comments
- WHEN: "When Claude needs to work with... for: (1)... (2)... (3)..."
- KEYWORDS: .docx files, tracked changes, professional documents

**Poor description** (missing elements):
```yaml
description: "处理文档相关功能"
```

Problems:
- WHAT: vague ("文档相关功能" — what specifically?)
- WHEN: missing (when should Agent use this?)
- KEYWORDS: missing (no ".docx", no specific scenarios)

**Another poor example**:
```yaml
description: "A helpful skill for various tasks"
```

This is useless — Agent has no idea when to activate it.

---

**Description quality checklist**:
- [ ] Lists specific capabilities (not just "helps with X")
- [ ] Includes explicit trigger scenarios ("Use when...", "When user asks for...")
- [ ] Contains searchable keywords (file extensions, domain terms, action verbs)
- [ ] Specific enough that Agent knows EXACTLY when to use it
- [ ] Includes scenarios where this skill MUST be used (not just "can be used")

---

### D5: Progressive Disclosure (15 points)

Does the Skill implement proper content layering?

Skill loading has three layers:
```
Layer 1: Metadata (always in memory)
         Only name + description
         ~100 tokens per skill

Layer 2: SKILL.md Body (loaded after triggering)
         Detailed guidelines, code examples, decision trees
         Ideal: < 500 lines

Layer 3: Resources (loaded on demand)
         scripts/, references/, assets/
         No limit
```

| Score | Criteria |
|-------|----------|
| 0-5 | Everything dumped in SKILL.md (>500 lines, no structure) |
| 6-10 | Has references but unclear when to load them |
| 11-13 | Good layering with MANDATORY triggers present |
| 14-15 | Perfect: decision trees + explicit triggers + "Do NOT Load" guidance |

**For Skills WITH references directory**, check Loading Trigger Quality:

| Trigger Quality | Characteristics |
|-----------------|-----------------|
| Poor | References listed at end, no loading guidance |
| Mediocre | Some triggers but not embedded in workflow |
| Good | MANDATORY triggers in workflow steps |
| Excellent | Scenario detection + conditional triggers + "Do NOT Load" |

**The loading problem**:
```
Loading too little ◄─────────────────────────────────► Loading too much
- References sit unused                    - Wastes context space
- Agent doesn't know when to load          - Irrelevant info dilutes key content
- Knowledge is there but never accessed    - Unnecessary token overhead
```

**Good loading trigger** (embedded in workflow):
```markdown
### Creating New Document

**MANDATORY - READ ENTIRE FILE**: Before proceeding, you MUST read
[`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish.
**NEVER set any range limits when reading this file.**

**Do NOT load** `ooxml.md` or `redlining.md` for this task.
```

**Bad loading trigger** (just listed):
```markdown
## References
- docx-js.md - for creating documents
- ooxml.md - for editing
- redlining.md - for tracking changes
```

**For simple Skills** (no references, <100 lines): Score based on conciseness and self-containment.

---

### D6: Freedom Calibration (15 points)

Is the level of specificity appropriate for the task's fragility?

Different tasks need different levels of constraint. This is about matching freedom to fragility.

| Score | Criteria |
|-------|----------|
| 0-5 | Severely mismatched (rigid scripts for creative tasks, vague for fragile ops) |
| 6-10 | Partially appropriate, some mismatches |
| 11-13 | Good calibration for most scenarios |
| 14-15 | Perfect freedom calibration throughout |

**The freedom spectrum**:

| Task Type | Should Have | Why | Example Skill |
|-----------|-------------|-----|---------------|
| Creative/Design | High freedom | Multiple valid approaches, differentiation is value | frontend-design |
| Code review | Medium freedom | Principles exist but judgment required | code-review |
| File format operations | Low freedom | One wrong byte corrupts file, consistency critical | docx, xlsx, pdf |

**High freedom** (text-based instructions):
```markdown
Commit to a BOLD aesthetic direction. Pick an extreme: brutally minimal,
maximalist chaos, retro-futuristic, organic natural...
```

**Medium freedom** (pseudocode or parameterized):
```markdown
Review priority:
1. Security vulnerabilities (must fix)
2. Logic errors (must fix)
3. Performance issues (should fix)
4. Maintainability (optional)
```

**Low freedom** (specific scripts, exact steps):
```markdown
**MANDATORY**: Use exact script in `scripts/create-doc.py`
Parameters: --title "X" --author "Y"
Do NOT modify the script.
```

**The test**: Ask "if Agent makes a mistake, what's the consequence?"
- High consequence → Low freedom
- Low consequence → High freedom

---

### D7: Pattern Recognition (10 points)

Does the Skill follow an established official pattern?

Through analyzing 17 official Skills, we identified 5 main design patterns:

| Pattern | ~Lines | Key Characteristics | Example | When to Use |
|---------|--------|---------------------|---------|-------------|
| **Mindset** | ~50 | Thinking > technique, strong NEVER list, high freedom | frontend-design | Creative tasks requiring taste |
| **Navigation** | ~30 | Minimal SKILL.md, routes to sub-files | internal-comms | Multiple distinct scenarios |
| **Philosophy** | ~150 | Two-step: Philosophy → Express, emphasizes craft | canvas-design | Art/creation requiring originality |
| **Process** | ~200 | Phased workflow, checkpoints, medium freedom | mcp-builder | Complex multi-step projects |
| **Tool** | ~300 | Decision trees, code examples, low freedom | docx, pdf, xlsx | Precise operations on specific formats |

| Score | Criteria |
|-------|----------|
| 0-3 | No recognizable pattern, chaotic structure |
| 4-6 | Partially follows a pattern with significant deviations |
| 7-8 | Clear pattern with minor deviations |
| 9-10 | Masterful application of appropriate pattern |

**Pattern selection guide**:

| Your Task Characteristics | Recommended Pattern |
|---------------------------|---------------------|
| Needs taste and creativity | Mindset (~50 lines) |
| Needs originality and craft quality | Philosophy (~150 lines) |
| Has multiple distinct sub-scenarios | Navigation (~30 lines) |
| Complex multi-step project | Process (~200 lines) |
| Precise operations on specific format | Tool (~300 lines) |

---

### D8: Practical Usability (15 points)

Can an Agent actually use this Skill effectively?

| Score | Criteria |
|-------|----------|
| 0-5 | Confusing, incomplete, contradictory, or untested guidance |
| 6-10 | Usable but with noticeable gaps |
| 11-13 | Clear guidance for common cases |
| 14-15 | Comprehensive coverage including edge cases and error handling |

**Check for**:
- **Decision trees**: For multi-path scenarios, is there clear guidance on which path to take?
- **Code examples**: Do they actually work? Or are they pseudocode that breaks?
- **Error handling**: What if the main approach fails? Are fallbacks provided?
- **Edge cases**: Are unusual but realistic scenarios covered?
- **Actionability**: Can Agent immediately act, or needs to figure things out?

**Good usability** (decision tree + fallback):
```markdown
| Task | Primary Tool | Fallback | When to Use Fallback |
|------|-------------|----------|----------------------|
| Read text | pdftotext | PyMuPDF | Need layout info |
| Extract tables | camelot-py | tabula-py | camelot fails |

**Common issues**:
- Scanned PDF: pdftotext returns blank → Use OCR first
- Encrypted PDF: Permission error → Use PyMuPDF with password
```

**Poor usability** (vague):
```markdown
Use appropriate tools for PDF processing.
Handle errors properly.
Consider edge cases.
```

---

## NEVER Do When Evaluating

- **NEVER** give high scores just because it "looks professional" or is well-formatted
- **NEVER** ignore token waste — every redundant paragraph should result in deduction
- **NEVER** let length impress you — a 43-line Skill can outperform a 500-line Skill
- **NEVER** skip mentally testing the decision trees — do they actually lead to correct choices?
- **NEVER** forgive explaining basics with "but it provides helpful context"
- **NEVER** overlook missing anti-patterns — if there's no NEVER list, that's a significant gap
- **NEVER** assume all procedures are valuable — distinguish domain-specific from generic
- **NEVER** undervalue the description field — poor description = skill never gets used
- **NEVER** put "when to use" info only in the body — Agent only sees description before loading

---

## Evaluation Protocol

### Step 1: First Pass — Knowledge Delta Scan

Read SKILL.md completely and for each section ask:
> "Does Claude already know this?"

Mark each section as:
- **[E] Expert**: Claude genuinely doesn't know this — value-add
- **[A] Activation**: Claude knows but brief reminder is useful — acceptable
- **[R] Redundant**: Claude definitely knows this — should be deleted

Calculate rough ratio: E:A:R
- Good Skill: >70% Expert, <20% Activation, <10% Redundant
- Mediocre Skill: 40-70% Expert, high Activation
- Bad Skill: <40% Expert, high Redundant

### Step 2: Structure Analysis

```
[ ] Check frontmatter validity
[ ] Count total lines in SKILL.md
[ ] List all reference files and their sizes
[ ] Identify which pattern the Skill follows
[ ] Check for loading triggers (if references exist)
```

### Step 3: Score Each Dimension

For each of the 8 dimensions:
1. Find specific evidence (quote relevant lines)
2. Assign score with one-line justification
3. Note specific improvements if score < max

### Step 4: Calculate Total & Grade

```
Total = D1 + D2 + D3 + D4 + D5 + D6 + D7 + D8
Max = 120 points
```

**Grade Scale** (percentage-based):
| Grade | Percentage | Meaning |
|-------|------------|---------|
| A | 90%+ (108+) | Excellent — production-ready expert Skill |
| B | 80-89% (96-107) | Good — minor improvements needed |
| C | 70-79% (84-95) | Adequate — clear improvement path |
| D | 60-69% (72-83) | Below Average — significant issues |
| F | <60% (<72) | Poor — needs fundamental redesign |

### Step 5: Generate Report

```markdown
# Skill Evaluation Report: [Skill Name]

## Summary
- **Total Score**: X/120 (X%)
- **Grade**: [A/B/C/D/F]
- **Pattern**: [Mindset/Navigation/Philosophy/Process/Tool]
- **Knowledge Ratio**: E:A:R = X:Y:Z
- **Verdict**: [One sentence assessment]

## Dimension Scores

| Dimension | Score | Max | Notes |
|-----------|-------|-----|-------|
| D1: Knowledge Delta | X | 20 | |
| D2: Mindset vs Mechanics | X | 15 | |
| D3: Anti-Pattern Quality | X | 15 | |
| D4: Specification Compliance | X | 15 | |
| D5: Progressive Disclosure | X | 15 | |
| D6: Freedom Calibration | X | 15 | |
| D7: Pattern Recognition | X | 10 | |
| D8: Practical Usability | X | 15 | |

## Critical Issues
[List must-fix problems that significantly impact the Skill's effectiveness]

## Top 3 Improvements
1. [Highest impact improvement with specific guidance]
2. [Second priority improvement]
3. [Third priority improvement]

## Detailed Analysis
[For each dimension scoring below 80%, provide:
- What's missing or problematic
- Specific examples from the Skill
- Concrete suggestions for improvement]
```

---

## Common Failure Patterns

### Pattern 1: The Tutorial
```
Symptom: Explains what PDF is, how Python works, basic library usage
Root cause: Author assumes Skill should "teach" the model
Fix: Claude already knows this. Delete all basic explanations.
     Focus on expert decisions, trade-offs, and anti-patterns.
```

### Pattern 2: The Dump
```
Symptom: SKILL.md is 800+ lines with everything included
Root cause: No progressive disclosure design
Fix: Core routing and decision trees in SKILL.md (<300 lines ideal)
     Detailed content in references/, loaded on-demand
```

### Pattern 3: The Orphan References
```
Symptom: References directory exists but files are never loaded
Root cause: No explicit loading triggers
Fix: Add "MANDATORY - READ ENTIRE FILE" at workflow decision points
     Add "Do NOT Load" to prevent over-loading
```

### Pattern 4: The Checkbox Procedure
```
Symptom: Step 1, Step 2, Step 3... mechanical procedures
Root cause: Author thinks in procedures, not thinking frameworks
Fix: Transform into "Before doing X, ask yourself..."
     Focus on decision principles, not operation sequences
```

### Pattern 5: The Vague Warning
```
Symptom: "Be careful", "avoid errors", "consider edge cases"
Root cause: Author knows things can go wrong but hasn't articulated specifics
Fix: Specific NEVER list with concrete examples and non-obvious reasons
     "NEVER use X because [specific problem that takes experience to learn]"
```

### Pattern 6: The Invisible Skill
```
Symptom: Great content but skill rarely gets activated
Root cause: Description is vague, missing keywords, or lacks trigger scenarios
Fix: Description must answer WHAT, WHEN, and include KEYWORDS
     "Use when..." + specific scenarios + searchable terms

Example fix:
BAD:  "Helps with document tasks"
GOOD: "Create, edit, and analyze .docx files. Use when working with
       Word documents, tracked changes, or professional document formatting."
```

### Pattern 7: The Wrong Location
```
Symptom: "When to use this Skill" section in body, not in description
Root cause: Misunderstanding of three-layer loading
Fix: Move all triggering information to description field
     Body is only loaded AFTER triggering decision is made
```

### Pattern 8: The Over-Engineered
```
Symptom: README.md, CHANGELOG.md, INSTALLATION_GUIDE.md, CONTRIBUTING.md
Root cause: Treating Skill like a software project
Fix: Delete all auxiliary files. Only include what Agent needs for the task.
     No documentation about the Skill itself.
```

### Pattern 9: The Freedom Mismatch
```
Symptom: Rigid scripts for creative tasks, vague guidance for fragile operations
Root cause: Not considering task fragility
Fix: High freedom for creative (principles, not steps)
     Low freedom for fragile (exact scripts, no parameters)
```

---

## Quick Reference Checklist

```
┌─────────────────────────────────────────────────────────────────────────┐
│  SKILL EVALUATION QUICK CHECK                                           │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  KNOWLEDGE DELTA (most important):                                      │
│    [ ] No "What is X" explanations for basic concepts                   │
│    [ ] No step-by-step tutorials for standard operations                │
│    [ ] Has decision trees for non-obvious choices                       │
│    [ ] Has trade-offs only experts would know                           │
│    [ ] Has edge cases from real-world experience                        │
│                                                                         │
│  MINDSET + PROCEDURES:                                                  │
│    [ ] Transfers thinking patterns (how to think about problems)        │
│    [ ] Has "Before doing X, ask yourself..." frameworks                 │
│    [ ] Includes domain-specific procedures Claude wouldn't know         │
│    [ ] Distinguishes valuable procedures from generic ones              │
│                                                                         │
│  ANTI-PATTERNS:                                                         │
│    [ ] Has explicit NEVER list                                          │
│    [ ] Anti-patterns are specific, not vague                            │
│    [ ] Includes WHY (non-obvious reasons)                               │
│                                                                         │
│  SPECIFICATION (description is critical!):                              │
│    [ ] Valid YAML frontmatter                                           │
│    [ ] name: lowercase, ≤64 chars                                       │
│    [ ] description answers: WHAT does it do?                            │
│    [ ] description answers: WHEN should it be used?                     │
│    [ ] description contains trigger KEYWORDS                            │
│    [ ] description is specific enough for Agent to know when to use     │
│                                                                         │
│  STRUCTURE:                                                             │
│    [ ] SKILL.md < 500 lines (ideal < 300)                               │
│    [ ] Heavy content in references/                                     │
│    [ ] Loading triggers embedded in workflow                            │
│    [ ] Has "Do NOT Load" for preventing over-loading                    │
│                                                                         │
│  FREEDOM:                                                               │
│    [ ] Creative tasks → High freedom (principles)                       │
│    [ ] Fragile operations → Low freedom (exact scripts)                 │
│                                                                         │
│  USABILITY:                                                             │
│    [ ] Decision trees for multi-path scenarios                          │
│    [ ] Working code examples                                            │
│    [ ] Error handling and fallbacks                                     │
│    [ ] Edge cases covered                                               │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## The Meta-Question

When evaluating any Skill, always return to this fundamental question:

> **"Would an expert in this domain, looking at this Skill, say:**
> **'Yes, this captures knowledge that took me years to learn'?"**

If the answer is yes → the Skill has genuine value.
If the answer is no → it's compressing what Claude already knows.

The best Skills are **compressed expert brains** — they take a designer's 10 years of aesthetic accumulation and compress it into 43 lines, or a document expert's operational experience into a 200-line decision tree.

What gets compressed must be things Claude doesn't have. Otherwise, it's garbage compression.

---

## Self-Evaluation Note

This Skill (skill-judge) should itself pass evaluation:

- **Knowledge Delta**: Provides specific evaluation criteria Claude wouldn't generate on its own
- **Mindset**: Shapes how to think about Skill quality, not just checklist items
- **Anti-Patterns**: "NEVER Do When Evaluating" section with specific don'ts
- **Specification**: Valid frontmatter with comprehensive description
- **Progressive Disclosure**: Self-contained, no external references needed
- **Freedom**: Medium freedom appropriate for evaluation task
- **Pattern**: Follows Tool pattern with decision frameworks
- **Usability**: Clear protocol, report template, quick reference



Evaluate this Skill against itself as a calibration exercise.


---

# Skill: skill-marketplace
Source: skills/skill-marketplace/SKILL.md

---
name: skill-marketplace
description: Install and manage skills/plugins via marketplaces and the add-skill workflow.
---
# Skill Marketplace

## When to use
- Installing or distributing skills/plugin packs for Kode-compatible agents

## Workflow
1. Add a marketplace (local path or GitHub shorthand zip).
2. Install plugin packs with scope: user (`~/.kode/...`) or project (`.kode/...`).
3. Enable/disable plugins per scope; list configured marketplaces.
4. For direct installs, use `add-skill` CLI to pull skills from any repo (optionally global, select skills by name).

## Anti-patterns
- Mixing scopes unintentionally (user vs project)
- Forgetting to enable after install
- Installing without verifying source trust

## Output
- Marketplace/skill actions performed, scope, and resulting installed/disabled/enabled state


---

# Skill: spec-writer
Source: skills/generated-agents/spec-writer/SKILL.md

name: spec-writer
description: Spec Writer — creates and updates project specifications and initializes projects from user prompts.
keywords: [spec-writer, spec, initialization]
source: partially-processed/agents/spec_writer.py

# Spec Writer

When to use
- When no project specification exists or when updating project specs based on user input.

Workflow
1. Prompt the user for project description (or use `args.initial_prompt`).
2. Use LLM to generate a full specification and a project name.
3. Initialize project state, file system, and epics; set `next_state.specification`.

Dependencies
- `core.templates.registry`, `core.llm.parser.StringParser`, telemetry, UI.

Output
- Sets `next_state.specification`, initializes project metadata and epics; returns special AgentResponse types when needed.


---

# Skill: spec-writer
Source: skills/spec-writer/SKILL.md

---
name: spec-writer
description: Collect project descriptions, refine specs, and update templates/complexity as the project evolves.
---
# Spec Writer

## When to use
- Initial project kickoff when no specification exists
- User edits require spec refinements or new feature incorporation
- Iterations request spec updates or project complexity reassessment

## Workflow
1. If the spec is empty, prompt (or reuse CLI args) for the project description, stream a detailed spec via `build_full_specification`, and derive a project name. Initialize project folders, update knowledge base options, and clone the spec into next_state.
2. For ongoing edits, loop with the user on "Are you satisfied?" until they confirm. Each time they add detail, regenerate the spec, update the current description, and keep the convo concise. When done, re-run `need_auth` to set auth flags and secrets, recompute complexity, refresh telemetry, and reset the initial epic content.
3. When iteration-mode updates arrive (e.g., NEW_FEATURE_REQUESTED), capture the feature description, stream the `add_new_feature` template, present a diff, and ask the user to accept. On approval, save the updated spec and move the iteration back to FIND_SOLUTION; otherwise leave the spec unchanged.
4. Ensure project templates run after spec creation when needed (`apply_template`), storing relevant files and template summaries.
5. After any change, propagate description/original_description, update knowledge base metadata, and emit project description updates via UI.

## Anti-patterns
- Overwriting user-edited specs without confirming satisfaction
- Skipping diff confirmation when adding new features
- Forgetting to flag knowledge base or epics when template/application metadata changes
- Leaving complexity or telemetry out of sync with the latest spec

## Output
- Updated specification text, complexity level, auth settings, project metadata (name, folder), template summary, and UI/telemetry signals marking spec progress


---

# Skill: standard-planning
Source: skills/standard-planning/SKILL.md

---
name: standard-planning
description: Balanced planning for routine feature development and maintenance.
keywords: [feature, maintenance, roadmap, enhancement]
---

# Standard Planning Workflow

## Steps
1. **Context Assessment**: Review existing related modules and documentation.
2. **Logical Mapping**: Outline the flow of data or control between components.
3. **Preparation Checklist**:
   - Environment variables needed?
   - New dependencies required?
   - Test coverage plan?
4. **Task Decomposition**: Create a linear todo list in the workspace state.

## Rules of Engagement
- Every plan must include a definition of "Done."
- Every plan must specify which existing files will be modified vs. created.


---

# Skill: system-architect
Source: skills/system-architect/SKILL.md

---
name: system-architect
description: High-level architectural analysis and structural design for modular codebases.
provenance: 
  source: "partially-processed/agents/architect.py"
  required_code: "partially-processed/tools/complexity_scanner/ (Metrics logic)"
---

# System Architecture Workflow

## 1. Structural Audit
- Map directory dependencies and cycles.
- Identify "God Classes" or overly coupled modules.

## 2. Design Pattern Proposal
- Select appropriate patterns (e.g., CQRS, Repository, Factory).
- Define interface boundaries between primary system components.

## 3. Scalability Mapping
- Plan for sharding, caching layers (Redis/Memcached), and async worker queues.
- Validate data flow against N-tier architecture standards.

## Standalone Requirements
- Architecture visualization tools found in `Coding_Tools/`.
- Complexity metric calculation scripts from `ai-logic/metrics/`.


---

# Skill: task-breakdown-developer
Source: skills/task-breakdown-developer/SKILL.md

---
name: task-breakdown-developer
description: Drive task execution by gathering context, confirming with the user, and producing actionable steps with file saves and commands.
---
# Task Breakdown Developer

## When to use
- An unfinished task or iteration needs detailed implementation steps
- The orchestrator hands off after specification and planning

## Workflow
1. If the current step requests a utility function, update the knowledge base and mark the step complete.
2. If iterations need attention, gather relevant files (bug hunt vs troubleshooting), parse human instructions, and convert them into next steps while updating iteration status (await logging/test vs complete).
3. For regular tasks:
   - Announce progress, ensure relevant files are fetched in parallel, and start the breakdown stream with highlighted API endpoints or redo feedback.
   - Generate implementation instructions via `breakdown` template, ensuring `<pythagoracode>` tags are balanced (retry up to two times); pass through chat-with-breakdown confirmation when auto-confirm is off.
   - Persist instructions onto the task, clear leftover modified files, and parse them with `parse_task` to create concrete steps.
4. Before executing, ask the user whether to run, edit, or skip the task unless flagged as always-run (quick implementations, user-added, hardcoded). Handle edits by resetting description and rerunning. Skips mark the task as skipped.
5. Convert parsed steps into next_state steps, preserving finished history, removing duplicate save_file entries, and set DEV_TASK_START/DEV_TROUBLESHOOT/DEV_WAIT_TEST actions as appropriate. Emit telemetry for task-start.

## Anti-patterns
- Skipping relevant file discovery before breakdown
- Ignoring redo feedback or iteration status transitions
- Proceeding with malformed breakdown output (mismatched `<pythagoracode>` tags)
- Forgetting to reset modified_files or flag tasks as modified after updates

## Output
- Updated task instructions, step list (commands, saves, interventions), current task status/action, and UI/telemetry signals indicating task progress


---

# Skill: task-completer
Source: skills/generated-agents/task-completer/SKILL.md

name: pyathagora-task-completer
description: Task Completer (Pythagora) — finalizes tasks, commits via Git when enabled, and signals completion.
keywords: [task-completer, commit, finish]
source: partially-processed/agents/task_completer.py

# Task Completer

When to use
- When a task is finished and the system should mark it as completed and optionally commit changes.

Workflow
1. If Git is available and used, run `git_commit()`.
2. Update `next_state` to mark task done, log completion, and send progress to UI.
3. If last task, emit app/feature finished events.

Dependencies
- `core.agents.git.GitMixin`, telemetry, UI.

Output
- Commits to Git (if configured), updates task status, and emits progress events.


---

# Skill: task-completer
Source: skills/task-completer/SKILL.md

---
name: task-completer
description: Finalize a task, commit changes when Git is active, and broadcast completion status.
---
# Task Completer

## When to use
- A development task reaches the end of its iteration and needs to be marked done
- Git commits should be created before transitioning to the next task

## Workflow
1. If Git is configured and used, trigger `git_commit()` via the GitMixin to capture work-in-progress before closing the task.
2. Calculate the 1-based index for the current task and set the next_state action using `TC_TASK_DONE`.
3. Call `next_state.complete_task()` and log completion through the state manager to persist history/telemetry.
4. Send task progress to the UI (task index, total tasks, description, source, status, and list of tasks) and emit telemetry `task-end` with project metadata.
5. When the final task within an epic finishes, notify the UI with either `send_app_finished` or `send_feature_finished` depending on epic source.

## Anti-patterns
- Skipping the Git commit before closing when repository tracking is active
- Forgetting to log task completion or update telemetry
- Failing to notify UI of completion, leaving inconsistent task state indicators

## Output
- Updated task status, logged completion, optional Git commit, UI progress message, and final completion signals when epics conclude


---

# Skill: task-troubleshooter
Source: skills/task-troubleshooter/SKILL.md

---
name: task-troubleshooter
description: Gather testing feedback, classify issues, and drive iteration loops until tasks are reviewed.
---
# Task Troubleshooter

## When to use
- After a developer completes implementation and the task needs testing feedback
- When iterations must gather user bug reports, change requests, or quick redoes

## Workflow
1. If an iteration is already in FIND_SOLUTION, reuse stored feedback and call `find_solution` to draft implementation instructions, then set status to IMPLEMENT_SOLUTION.
2. Otherwise, bootstrap a new iteration:
   - Determine (or infer) the run command, fetching via `get_run_command` when missing.
   - If task lacks testing instructions, generate them from route files and save them before requesting feedback.
   - Present test instructions to the user, along with rerun/run-command hooks.
3. Solicit user feedback using TS prompts: Everything works, There is an issue, I want to change spec, or Redo task.
   - Handle redo requests by capturing additional human instructions and exiting so the orchestrator reloads earlier state.
   - For change/bug paths, gather detailed feedback, fetch relevant files, and classify iteration status (NEW_FEATURE_REQUESTED vs HUNTING_FOR_BUG).
4. Detect loops using iteration counts; set status to PROBLEM_SOLVER when loops recur and trace telemetry events.
5. Append the new iteration to next_state with proper metadata (user_feedback, attempts, status, bug_hunting_cycles) and flag modifications. When no further work is required, mark the task reviewed, set TS_TASK_REVIEWED, and end the loop.

## Anti-patterns
- Skipping run command discovery, leaving users without execution instructions
- Failing to store test instructions or relevant files for subsequent iterations
- Ignoring loop detection, starving ProblemSolver of alternative solutions
- Leaving iteration status unset when user requests a spec change or reports a bug

## Output
- Updated iterations list (feedback, status, attempts), task test instructions, run command, and UI actions such as TS_TASK_REVIEWED or TS_ALT_SOLUTION


---

# Skill: tech-lead
Source: skills/generated-agents/tech-lead/SKILL.md

name: tech-lead
description: Tech Lead — creates development plans, epics and breaks down features into tasks.
keywords: [tech-lead, planning, epics]
source: partially-processed/agents/tech_lead.py

# Tech Lead

When to use
- When initial project planning or feature breakdown is required; to produce epics and task-level plans.

Workflow
1. Create initial project epic when bootstrapping.
2. Apply project templates if available and produce relevant file modifications.
3. Produce development plans and epics using LLM-driven templates and update `next_state.epics`.

Dependencies
- `core.templates.registry.PROJECT_TEMPLATES`, telemetry, `core.db.models.Complexity`.

Output
- Updates `next_state.epics`, may set `next_state.relevant_files` and `next_state.modified_files`.


---

# Skill: tech-lead-planner
Source: skills/tech-lead-planner/SKILL.md

---
name: tech-lead-planner
description: Manage epics, plans, and task generation for new features while keeping project state clean.
---
# Tech Lead Planner

## When to use
- After specification to spin up the first development plan and tasks
- When a feature epic is active and needs sub-epics/tasks
- When the user asks for a new feature or quick implementation

## Workflow
1. On first run with only the bootstrap epic, clear mocked data, clone the spec, and add the initial "Build frontend" epic; reset relevant/modified file markers.
2. If templates exist and the project just started, optionally apply project templates (respect summaries, maintain relevant files empty afterward).
3. When an epic is active:
   - For feature epics, fetch relevant files in parallel before planning.
   - Use TECH_LEAD_PLANNING to draft the epic-level development plan; choose sub-epics depending on feature vs app complexity.
   - For each sub-epic, call TECH_LEAD_EPIC_BREAKDOWN to form tasks, concatenating descriptions, related API endpoints, and testing instructions.
   - Append generated tasks to next state, emit epics/tasks to the UI twice (before and after update), flag modifications, and log telemetry.
4. When no epic remains, prompt the user for next work:
   - If the user wants a new feature, create a feature epic, set TL_START_FEATURE action, and hand control back to orchestrator.
   - For quick implementations, resurrect the previous state epics/tasks, trim old logs, append a new task under the last sub-epic, and flag tasks/epics modified before notifying the UI.
5. Whenever auth is enabled on the initial app, inject the login/register hard-coded task at the front, prime log streams, and emit starting task/project-stage signals.

## Anti-patterns
- Leaving mocked data intact after bootstrapping
- Skipping relevant-file gathering for feature planning
- Forgetting to flag tasks/epics as modified after mutating lists
- Dropping template summaries or telemetry updates when templates are applied
- Adding quick implementation tasks without trimming previous logs

## Output
- Updated epics, sub-epics, and task lists (with descriptions, API endpoints, testing instructions)
- Project stage/action updates (TL_CREATE_PLAN, TL_START_FEATURE, etc.) and telemetry events reflecting the plan creation


---

# Skill: tech-writer
Source: skills/generated-agents/tech-writer/SKILL.md

name: tech-writer
description: Technical Writer — creates documentation (README, docs) after task completion.
keywords: [tech-writer, docs, README]
source: partially-processed/agents/tech_writer.py

# Technical Writer

When to use
- After tasks or epics finish to generate user-facing documentation, README, and project notes.

Workflow
1. Trigger on task completion thresholds or explicit requests.
2. Generate README or other docs via LLM prompts and save files via `state_manager.save_file`.
3. Mark tasks as documented in `next_state`.

Dependencies
- UI helpers, telemetry, `core.ui.base.success_source`.

Output
- Creates/updates documentation files (e.g., `README.md`), sets task status to DOCUMENTED.


---

# Skill: tech-writer
Source: skills/tech-writer/SKILL.md

---
name: tech-writer
description: Celebrate project milestones, generate documentation, and mark tasks as documented.
---
# Tech Writer

## When to use
- After Troubleshooter marks a task as reviewed and documentation should follow
- At key milestones (halfway, final task) to keep users informed and motivated

## Workflow
1. Count total tasks and unfinished tasks (minus the current one) to locate milestones; when halfway or one task remains, send a celebration update and summarize progress stats.
2. When triggered, stream a README draft via the `create_readme` template and save it to README.md.
3. Set TW_WRITE action, update current task status to DOCUMENTED, and allow orchestrator to proceed.

## Anti-patterns
- Emitting congratulations without verifying task counts (avoids division by zero)
- Forgetting to decrement unfinished tasks before milestone checks
- Skipping README generation when required

## Output
- Updated task status (documented), optional celebratory message, and fresh README content stored in the repo


---

# Skill: telemetry-kusto-analyst
Source: skills/telemetry-kusto-analyst/SKILL.md

---
name: telemetry-kusto-analyst
description: Answer VS Code telemetry questions by running real Kusto queries with best practices.
---
# Telemetry Kusto Analyst

## When to use
- Investigating VS Code telemetry using KQL and providing real query results

## Workflow
1. If missing, fetch telemetry docs (`vscode-telemetry-docs/.github/copilot-instructions.md`).
2. If needed, install/ensure Kusto tooling (`kusto_query` tool or Azure MCP extension).
3. Run real Kusto queries (default rolling 28-day window if not specified); include proper time filters.
4. Parallelize independent queries when useful; stop after repeated failures and report.
5. Present query text, results, and interpretation; cite docs when relevant and scrub user-identifiable data.

## Anti-patterns
- Describing hypothetical queries instead of running them
- Omitting time filters or using partial-day windows
- Ignoring repeated query failures without reporting

## Output
- Executed Kusto query, results, and a concise analysis answering the question


---

# Skill: thinkpdf
Source: skills/thinkpdf/SKILL.md

---
name: thinkpdf
description: Convert PDFs to structured markdown, invoke OCR when needed, and expose tools via CLI, GUI, or MCP server.
---
# thinkpdf

## When to use
- Extracting text, tables, or structure from PDFs for RAG pipelines or LLM context
- Batch converting folders of PDFs with progress reporting and caching
- Wiring PDF utilities into IDEs via the built-in MCP server or launching a lightweight GUI

## Workflow
1. Install the package (`pip install thinkpdf`) and add extras when required:
   - `thinkpdf[docling]` for higher-fidelity table extraction
   - `thinkpdf[gui]` to launch the desktop UI with `thinkpdf-gui`
2. Use the CLI (`thinkpdf`) for conversions:
   - Single file: `thinkpdf input.pdf [-o output.md]`
   - Batch: `thinkpdf folder --batch [--workers N]` with optional caching (`--no-cache` disables) and OCR toggles (`--ocr off|auto|force`)
   - Setup MCP config: `thinkpdf setup` prints the JSON block for Cursor/Antigravity
3. Programmatic usage: `from thinkpdf import convert; convert("document.pdf")` returns markdown. Adjust `thinkpdf.core.models.Options` for OCR mode or asset export as needed.
4. For GUI workflows, install the `[gui]` extra then run `thinkpdf-gui` to open the Qt interface (reads/writes markdown alongside assets).
5. To integrate with MCP-compatible editors, add the provided block to `mcp.json` so tools become available:
   - `read_pdf` streams PDF content
   - `convert_pdf` saves markdown outputs
   - `get_document_info` returns metadata
6. When running the CLI, include `--export-images` to collect embedded images into an `_assets` folder and `--password` for encrypted PDFs. Enable `-v/--verbose` to stream progress callbacks.
7. Review cached results stored by `CacheManager()` (default unless `--no-cache`) to avoid redundant conversions; clear cache manually if source PDFs change.

## Anti-patterns
- Forgetting to install the `[docling]` extra when table fidelity matters
- Running `--batch` on a single file path (requires a directory)
- Skipping MCP setup after printing the JSON block, leading to missing IDE tools
- Leaving cache enabled when source PDFs change (run with `--no-cache` or purge cache first)

## Output
- Markdown files (and optional assets) generated from PDFs, accessible via CLI, GUI, library calls, or MCP tools ready for downstream RAG ingestion


---

# Skill: token-usage-monitor
Source: skills/token-usage-monitor/SKILL.md

---
name: token-usage-monitor
description: Track per-step token counts, image costs, and cumulative spend for GPT Engineer runs.
---
# Token Accounting

## When to use
- You need observability into prompt/completion totals while orchestrating multi-step conversations
- A run mixes text and vision payloads and you must estimate the combined OpenAI usage cost
- CI should emit CSV summaries of token consumption after each agent stage

## Workflow
1. Initialize `TokenUsageLog(model_name)` to seed cumulative counters and a tokenizer tuned to the target model [partially-processed/core/token_usage.py#L195-L207](partially-processed/core/token_usage.py#L195-L207).
2. For each step, call `update_log(messages, answer, step_name)`; it tallies prompt and completion counts via the tokenizer, accumulates totals, and appends a `TokenUsage` snapshot [partially-processed/core/token_usage.py#L208-L239](partially-processed/core/token_usage.py#L208-L239).
3. Behind the scenes, `Tokenizer.num_tokens_from_messages` handles both plain text and base64 image payloads, pricing tiled images per OpenAI's vision rules [partially-processed/core/token_usage.py#L82-L192](partially-processed/core/token_usage.py#L82-L192).
4. Retrieve structured records with `log()` or emit a CSV using `format_log()` when you need artifacts for dashboards [partially-processed/core/token_usage.py#L241-L265](partially-processed/core/token_usage.py#L241-L265).
5. To estimate spend, call `usage_cost()`; it looks up per-model pricing and sums prompt/completion costs when the model string contains `gpt` [partially-processed/core/token_usage.py#L266-L312](partially-processed/core/token_usage.py#L266-L312).

## Anti-patterns
- Passing messages that skip the framing tokens (e.g., raw strings); wrap them in LangChain `AIMessage` or `HumanMessage` so token counting stays accurate
- Feeding non-base64 image URLs; the tokenizer expects inline data to measure resolution tiles
- Mixing multiple models inside one log; instantiate a new `TokenUsageLog` per model to keep pricing correct
- Suppressing cost calculation errors silently; piping stdout to logs keeps pricing anomalies visible

## Output
- Step-by-step token usage records, CSV exports, and optional USD cost projections that inform rate-limited planning


---

# Skill: tools-overview
Source: skills/tools-overview/SKILL.md

---
name: tools-overview
description: High-level guide to repository utility scripts and tool entrypoints.
---
# Tools Overview

## When to use
- You want a quick map of utility scripts, services, and invocation patterns in the repo
- Agents need to pick the right helper tool for tasks like fetching docs, running linters, or packaging assets

## Workflow
1. Scan `Tools/` and all `tools*` directories for executable scripts and tool manifests.
2. Record CLI signatures and environment expectations so agents can call them with correct flags.
3. Prefer documented entrypoints; if missing, add a short `--help` extraction step to the tool inventory.

## Output
- A curated index of scripts with sample invocations, expected inputs, and side effects for safe agent use


---

# Skill: tpilotx-cli
Source: skills/tpilotx-cli/SKILL.md

---
name: tpilotx-cli
description: Launch and control the TpilotX/Claude CLI runner, wiring config, database, UI adapters, and cleanup so the orchestrator can execute safely.
---
# TpilotX CLI

## When to use
- You need to start a full Pythagora/TpilotX session from the command line, either for new builds or to resume saved projects
- Operators must list, inspect, or delete stored projects/branches before re-running the orchestrator
- You are packaging the CLI for desktop extensions or remote IPC clients and must configure logging, telemetry, and API endpoints correctly

## Workflow
1. Parse command arguments (`helpers.parse_arguments`) so flags like `--project`, `--list`, `--llm-endpoint`, or `--no-auto-confirm-breakdown` are available. The parser also exposes feature toggles for IPC (`--enable-api-server`), custom config paths, and explicit stack prompts.
2. Load configuration via `helpers.load_config`, allowing `.env` fallbacks. Apply CLI overrides for log level, DB URL, IPC host/port, and provider base URLs/keys. Validate with `Config.model_validate`; bail out early if parsing fails.
3. Initialize logging and choose the UI adapter: IPC client, scripted `VirtualUI`, or default `PlainConsoleUI`. Run `run_migrations` against the configured database and create a `SessionManager` bound to the parsed args.
4. Call `async_main` (through `run_tpilotx`) to dispatch control flow: quick-return for listing (`--list`, `--list-json`), config display, legacy imports, or project deletes. Otherwise instantiate `StateManager`, optional `IPCServer`, apply access tokens, telemetry, and Sentry if enabled.
5. For fresh sessions, `start_new_project` walks the UI question flow (stack selection, prompt capture). For resumptions, `load_project` restores the requested project/branch/step and rehydrates FE/BE logs plus conversations before handing off to the `Orchestrator`.
6. Register signal handlers and `atexit` cleanup to flush telemetry and shut down the UI. On teardown, call `cleanup(ui)`, stop any API server, and ensure the UI closes cleanly before exiting the process.

## Anti-patterns
- Skipping `run_migrations`, which leaves the orchestrator working against stale schemas
- Providing both `--project` and `--branch` without matching IDs, causing confusing "not found" errors instead of checking `--list`
- Forgetting to run with `--no-auto-confirm-breakdown` when testing human-in-the-loop flows, leading to unintended auto approvals
- Ignoring the need to call `capture_exception`/`send_error`; unhandled exceptions bypass UI notifications and telemetry crash reports

## Output
- A running CLI session with the selected UI, database migrations applied, telemetry and optional API server active, and orchestrator control handed over to manage tasks or project resumptions


---

# Skill: troubleshooter
Source: skills/generated-agents/troubleshooter/SKILL.md

name: troubleshooter
description: Troubleshooter — creates debugging/troubleshooting iterations and proposes solutions.
keywords: [troubleshooter, debug, iteration]
source: partially-processed/agents/troubleshooter.py

# Troubleshooter

When to use
- When a task or iteration fails testing, or when user reports issues requiring investigation.

Workflow
1. Gather run command and user test instructions; if missing, request them.
2. Generate bug reports or alternative solutions using LLM prompts and helper mixins.
3. Create new iterations, update statuses (e.g., HUNTING_FOR_BUG, IMPLEMENT_SOLUTION), and coordinate testing.

Dependencies
- `core.agents.mixins` (ChatWithBreakdownMixin, IterationPromptMixin, RelevantFilesMixin), `core.llm.parser`.

Output
- Appends new iterations to `next_state.iterations`, updates `next_state.current_iteration` and `next_state.action`.


---

# Skill: type-safety-enforcer
Source: skills/type-safety-enforcer/SKILL.md

---
name: type-safety-enforcer
description: Eliminates technical debt by enforcing strict type definitions and avoiding 'any'.
keywords: [typescript, type-safety, any, interfaces, types]
---

# Type Safety Enforcement

## The "Zero Any" Policy
- Use of `any` is a failure condition unless mandated by a specific library limitation.
- Use `unknown` with type guards if the type is truly dynamic.

## Workflow
1. **Interface Mapping**: Define the shape of all inputs and outputs before writing logic.
2. **Generics Implementation**: Use generics for reusable utility functions to maintain end-to-end safety.
3. **Type Narrowing**: Implement custom type guards (`isType`) for complex union types.

## Verification
- Run `tsc --noEmit` to confirm no hidden regressions.
- Ensure all external API responses are cast to validated interfaces.


---

# Skill: wizard
Source: skills/generated-agents/wizard/SKILL.md

name: wizard
description: Wizard — sets up initial project templates (frontend), handles OpenAPI uploads and project bootstrap.
keywords: [wizard, bootstrap, template]
source: partially-processed/agents/wizard.py

# Wizard

When to use
- During initial project setup to initialize templates, frontend epics, and project knowledge base.

Workflow
1. If project type is `swagger`, request OpenAPI/Swagger docs and upload them; otherwise set default options.
2. Initialize knowledge base and create initial frontend epic.
3. Return `AgentResponse.create_specification` to trigger Spec Writer flow.

Dependencies
- `core.db.models.KnowledgeBase`, `core.ui.base`, HTTP upload to RAG service (`PYTHAGORA_API`).

Output
- Initializes `next_state.epics`, `next_state.knowledge_base`, and may set template options.


---

